{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db3e419-c5bb-4325-b538-d9d7aa6e7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe2ec52-a7fb-4c7e-821c-c6b4addec4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "OUTPUT_DIR = \"./output/\"\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"generated_texts.json\")\n",
    "EVAL_FILE = os.path.join(OUTPUT_DIR, \"evaluation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3358f4-220d-4df5-85d6-c2a03fae2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    def __init__(self, json_file_path, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize the evaluation metrics class.\n",
    "\n",
    "        Args:\n",
    "            json_file_path (str): Path to the JSON file containing true and generated text.\n",
    "            output_dir (str): Directory to save the evaluation results.\n",
    "        \"\"\"\n",
    "        self.json_file_path = json_file_path\n",
    "        self.output_dir = output_dir\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        print(f\"Initialized EvaluationMetrics with JSON file: {self.json_file_path}\")\n",
    "        self.data = self._load_and_fix_json()\n",
    "\n",
    "    def _load_and_fix_json(self):\n",
    "        \"\"\"\n",
    "        Load and parse the JSON file, accessing the `generated_texts` field.\n",
    "\n",
    "        Returns:\n",
    "            list: Parsed JSON data as a list of dictionaries.\n",
    "        \"\"\"\n",
    "        print(f\"Loading and checking JSON file: {self.json_file_path}\")\n",
    "        with open(self.json_file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Ensure the key `generated_texts` is present and contains data\n",
    "        if \"generated_texts\" not in json_data or not isinstance(json_data[\"generated_texts\"], list):\n",
    "            raise ValueError(\"The JSON file does not contain a valid 'generated_texts' field.\")\n",
    "        \n",
    "        data = json_data[\"generated_texts\"]\n",
    "        print(f\"Loaded {len(data)} records from JSON file.\")\n",
    "        return data\n",
    "    \n",
    "    def compute_metrics(self, sample):\n",
    "        \"\"\"\n",
    "        Compute ROUGE and BLEU metrics for a single sample.\n",
    "\n",
    "        Args:\n",
    "            sample (dict): A dictionary containing \"true\" and \"generated\" fields.\n",
    "\n",
    "        Returns:\n",
    "            dict: Metrics including ROUGE-1, ROUGE-2, ROUGE-L, and BLEU scores.\n",
    "        \"\"\"\n",
    "        true_desc = sample[\"true\"]\n",
    "        gen_desc = sample[\"generated\"]\n",
    "\n",
    "        # ROUGE Scores\n",
    "        rouge_scores = self.scorer.score(true_desc, gen_desc)\n",
    "\n",
    "        # BLEU Score with smoothing (unigram and bigram)\n",
    "        smoothing_function = SmoothingFunction().method1\n",
    "        bleu_score = sentence_bleu(\n",
    "            [true_desc.split()],\n",
    "            gen_desc.split(),\n",
    "            weights=(0.5, 0.5),\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "\n",
    "        print(f\"Computed metrics for ID: {sample['id']} (Epoch: {sample['epoch']})\")\n",
    "        return {\n",
    "            \"epoch\": sample[\"epoch\"],\n",
    "            \"rouge1\": rouge_scores['rouge1'].fmeasure,\n",
    "            \"rouge2\": rouge_scores['rouge2'].fmeasure,\n",
    "            \"rougeL\": rouge_scores['rougeL'].fmeasure,\n",
    "            \"bleu\": bleu_score\n",
    "        }\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Perform evaluation by computing all metrics and saving results to a CSV file.\n",
    "\n",
    "        Returns:\n",
    "            str: Path to the saved CSV file.\n",
    "        \"\"\"\n",
    "        print(\"Starting evaluation process...\")\n",
    "        \n",
    "        # Compute metrics for each sample\n",
    "        print(\"Computing metrics for each sample...\")\n",
    "        metric_results = [self.compute_metrics(sample) for sample in self.data]\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        df = pd.DataFrame(metric_results)\n",
    "        print(f\"Computed metrics for all {len(self.data)} samples.\")\n",
    "\n",
    "        # Calculate averages per epoch\n",
    "        print(\"Calculating average metrics per epoch...\")\n",
    "        avg_per_epoch = df.groupby(\"epoch\").mean(numeric_only=True).reset_index()\n",
    "\n",
    "        # Save results with per-epoch averages\n",
    "        avg_per_epoch.to_csv(EVAL_FILE, index=False)\n",
    "        print(f\"Per-epoch metrics calculated and saved to {EVAL_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46cdb066-60f6-4546-a5ab-4f71c590506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized EvaluationMetrics with JSON file: ./output/generated_texts.json\n",
      "Loading and checking JSON file: ./output/generated_texts.json\n",
      "Fixed JSON formatting if necessary.\n",
      "Loaded 40 records from JSON file.\n",
      "Starting evaluation process...\n",
      "Cleaning 'generated' fields in the data...\n",
      "Cleaned 0 'generated' fields.\n",
      "Computing metrics for each sample...\n",
      "Computed metrics for ID: 0\n",
      "Computed metrics for ID: 1\n",
      "Computed metrics for ID: 2\n",
      "Computed metrics for ID: 3\n",
      "Computed metrics for ID: 4\n",
      "Computed metrics for ID: 5\n",
      "Computed metrics for ID: 6\n",
      "Computed metrics for ID: 7\n",
      "Computed metrics for ID: 8\n",
      "Computed metrics for ID: 9\n",
      "Computed metrics for ID: 10\n",
      "Computed metrics for ID: 11\n",
      "Computed metrics for ID: 12\n",
      "Computed metrics for ID: 13\n",
      "Computed metrics for ID: 14\n",
      "Computed metrics for ID: 15\n",
      "Computed metrics for ID: 16\n",
      "Computed metrics for ID: 17\n",
      "Computed metrics for ID: 18\n",
      "Computed metrics for ID: 19\n",
      "Computed metrics for ID: 0\n",
      "Computed metrics for ID: 1\n",
      "Computed metrics for ID: 2\n",
      "Computed metrics for ID: 3\n",
      "Computed metrics for ID: 4\n",
      "Computed metrics for ID: 5\n",
      "Computed metrics for ID: 6\n",
      "Computed metrics for ID: 7\n",
      "Computed metrics for ID: 8\n",
      "Computed metrics for ID: 9\n",
      "Computed metrics for ID: 10\n",
      "Computed metrics for ID: 11\n",
      "Computed metrics for ID: 12\n",
      "Computed metrics for ID: 13\n",
      "Computed metrics for ID: 14\n",
      "Computed metrics for ID: 15\n",
      "Computed metrics for ID: 16\n",
      "Computed metrics for ID: 17\n",
      "Computed metrics for ID: 18\n",
      "Computed metrics for ID: 19\n",
      "Computed metrics for all 40 samples.\n",
      "Calculating average metrics per epoch...\n",
      "Per-epoch metrics calculated and saved to ./output/evaluation_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./output/evaluation_metrics.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = EvaluationMetrics(OUTPUT_FILE, OUTPUT_DIR)\n",
    "\n",
    "# Run the evaluation\n",
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8545f-1609-4e2e-b09a-681f5aebd562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
