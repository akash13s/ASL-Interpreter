{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1463900a-a4a8-45a8-bfc4-af5b61da8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv\n",
    "from peft import PeftModel\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoProcessor, LlavaNextVideoForConditionalGeneration, BitsAndBytesConfig\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42fe3a26-75c6-4419-bd72-8cdb2afbf509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_ID = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n",
    "\n",
    "CACHE_DIR = \"./cache/\"\n",
    "VIDEO_DIR = \"/scratch/as18464/raw_videos\"\n",
    "CSV_FILE = \"../../data/valid_clips.csv\"\n",
    "OUTPUT_FILE = \"./output/inference_results.csv\"\n",
    "GENERATED_TEXTS = \"./output/generated_texts.csv\"\n",
    "\n",
    "# Model constants\n",
    "MAX_LENGTH = 3500  # Fixed sequence length for text\n",
    "NUM_FRAMES = 16  # Fixed number of frames\n",
    "IMAGE_SIZE = 224  # Fixed image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "578bb48e-107c-453c-aa31-a507f50f3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_config(use_qlora: bool, use_4bit: bool, use_8bit: bool, use_double_quant: bool):\n",
    "    \"\"\"\n",
    "    Generate the appropriate BitsAndBytesConfig for quantization.\n",
    "    \n",
    "    Args:\n",
    "        use_qlora (bool): Whether QLoRA-specific settings should be used.\n",
    "        use_4bit (bool): Enable 4-bit quantization.\n",
    "        use_8bit (bool): Enable 8-bit quantization.\n",
    "        use_double_quant (bool): Enable double quantization (QLoRA-specific).\n",
    "    \n",
    "    Returns:\n",
    "        BitsAndBytesConfig: Configured object for the quantization setup.\n",
    "    \"\"\"\n",
    "    # Validation to avoid conflicting quantization options\n",
    "    assert not (use_8bit and use_4bit), \"Cannot use both 8-bit and 4-bit quantization simultaneously.\"\n",
    "\n",
    "    # Base configuration\n",
    "    quantization_config = {\n",
    "        \"load_in_8bit\": use_8bit,\n",
    "        \"load_in_4bit\": use_4bit,\n",
    "        \"bnb_4bit_compute_dtype\": torch.float16\n",
    "    }\n",
    "\n",
    "    # Add QLoRA-specific options if enabled\n",
    "    if use_qlora:\n",
    "        quantization_config.update({\n",
    "            \"bnb_4bit_quant_type\": \"nf4\",\n",
    "            \"bnb_4bit_use_double_quant\": use_double_quant\n",
    "        })\n",
    "\n",
    "    return BitsAndBytesConfig(**quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f28e31b-9360-4b6a-a7a5-b78cbefe22a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "\n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            # Convert to numpy array in RGB format\n",
    "            frame_array = frame.to_ndarray(format=\"rgb24\")\n",
    "            # Apply resize transform and convert back to numpy\n",
    "            resized_frame = resize_transform(frame_array).numpy()\n",
    "            # Convert from CxHxW to HxWxC format and scale back to 0-255 range\n",
    "            resized_frame = (resized_frame.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "            frames.append(resized_frame)\n",
    "\n",
    "    return np.stack(frames)\n",
    "\n",
    "def get_frames(video_path: str, num_frames: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract frames from video with consistent sampling\n",
    "    Args:\n",
    "        video_path (str): Path to video file\n",
    "        num_frames (int): Number of frames to extract\n",
    "    Returns:\n",
    "        np.ndarray: Array of frames with shape (num_frames, height, width, 3)\n",
    "    \"\"\"\n",
    "    container = av.open(video_path)\n",
    "\n",
    "    # Get video stream\n",
    "    stream = container.streams.video[0]\n",
    "    total_frames = stream.frames\n",
    "\n",
    "    # Calculate indices to sample\n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "    # Read frames at calculated indices\n",
    "    frames = read_video_pyav(container, indices)\n",
    "\n",
    "    # Ensure we got exactly num_frames\n",
    "    if len(frames) < num_frames:\n",
    "        # If we got fewer frames, duplicate the last frame\n",
    "        last_frame = frames[-1]\n",
    "        while len(frames) < num_frames:\n",
    "            frames = np.concatenate([frames, last_frame[np.newaxis, ...]], axis=0)\n",
    "    elif len(frames) > num_frames:\n",
    "        # If we got more frames, take the first num_frames\n",
    "        frames = frames[:num_frames]\n",
    "\n",
    "    container.close()\n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45644ef1-a1d5-421d-95c7-ab5935e84e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for handling video data and corresponding text annotations.\n",
    "    Prepares video frames and text prompts for model input.\n",
    "\n",
    "    Args:\n",
    "        video_dir (str): Directory containing the video files.\n",
    "        annotations (pd.DataFrame): DataFrame containing video metadata and text annotations.\n",
    "        processor: Processor for tokenizing text and preparing video frames.\n",
    "        num_frames (int): Number of frames to extract from each video. Default is 16.\n",
    "        mode (str): Mode of the dataset, either \"train\" or \"eval\". Default is \"train\".\n",
    "                    If \"train\", the true sentence is included in the prompt. Otherwise, it is excluded.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_dir: str, annotations: pd.DataFrame, processor, num_frames: int = 16, mode: str = \"train\"):\n",
    "        self.video_dir = video_dir\n",
    "        self.annotations = annotations\n",
    "        self.num_frames = num_frames\n",
    "        self.processor = processor\n",
    "        self.mode = mode\n",
    "        self.system_prompt = (\"Analyze the American Sign Language (ASL) signs in this video and \"\n",
    "                              \"translate them into clear, natural English. Consider the sequence of \"\n",
    "                              \"signs as a complete message, and provide an accurate translation that \"\n",
    "                              \"captures the full meaning. Respond with only the English translation, \"\n",
    "                              \"without descriptions of the signs themselves.\")\n",
    "\n",
    "        print(f\"Created dataset split with {len(self.annotations)} entries\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: The length of the annotations DataFrame.\n",
    "        \"\"\"\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieves the video and text annotation for a given index, processes them into model input format.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing processed input tensors for the model:\n",
    "                - input_ids: Encoded text input IDs.\n",
    "                - attention_mask: Attention mask for text input.\n",
    "                - pixel_values_videos: Processed video frames as tensors.\n",
    "                - labels: Labels for supervised learning.\n",
    "                - video_id: The ID of the video (for later use in generation and evaluation).\n",
    "        \"\"\"\n",
    "        row = self.annotations.iloc[idx]\n",
    "        video_id = str(row['SENTENCE_NAME']).strip()\n",
    "        sentence = str(row['SENTENCE']).strip()\n",
    "\n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "        if not os.path.isfile(video_path):\n",
    "            raise FileNotFoundError(f\"Video file '{video_path}' not found.\")\n",
    "\n",
    "        # Get video frames using the provided functions\n",
    "        frames = get_frames(video_path, self.num_frames)\n",
    "\n",
    "        # Prepare the prompt\n",
    "        if self.mode == \"train\":\n",
    "            prompt = f\"USER: {self.system_prompt}\\n<video>\\nASSISTANT: {sentence}\"\n",
    "        else:\n",
    "            prompt = f\"USER: {self.system_prompt}\\n<video>\\nASSISTANT:\"  # Exclude true sentence\n",
    "\n",
    "        # Process the frames and text with fixed sizes\n",
    "        inputs = self.processor(\n",
    "            text=prompt,\n",
    "            videos=[frames],  # frames is already in the correct format from get_frames\n",
    "            padding=\"max_length\",  # Always pad to max_length\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = None\n",
    "        if self.mode == \"train\":\n",
    "            labels = self.get_labels(inputs)\n",
    "\n",
    "        # Return tensors with consistent sizes\n",
    "        item = {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"pixel_values_videos\": inputs[\"pixel_values_videos\"].squeeze(0),\n",
    "            \"video_id\": video_id,\n",
    "            \"true_sentence\": sentence\n",
    "        }\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            item[\"labels\"] = labels.squeeze(0)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def get_labels(self, inputs: dict) -> np.ndarray:\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        # Mask everything before and including \"ASSISTANT:\"\n",
    "        assistant_start = None\n",
    "        for j in range(len(inputs[\"input_ids\"][0])):\n",
    "            if self.processor.tokenizer.decode(inputs[\"input_ids\"][0][j:j + 4]) == \"ASSISTANT:\":\n",
    "                assistant_start = j\n",
    "                break\n",
    "\n",
    "        if assistant_start is not None:\n",
    "            labels[0, :assistant_start + 4] = -100\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "926bf2c3-1d7b-41cd-9e76-1db898ec16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_with_dataset(video_dir, csv_file, generated_texts_csv, output_file, processor, model, num_frames, device):\n",
    "    \"\"\"\n",
    "    Run inference using the VideoDataset and save results to a CSV file.\n",
    "    Filter video IDs based on the first epoch in generated_texts.csv.\n",
    "\n",
    "    Args:\n",
    "        video_dir (str): Path to the video directory.\n",
    "        csv_file (str): Path to the annotations CSV file.\n",
    "        generated_texts_csv (str): Path to the CSV file containing generated texts.\n",
    "        output_file (str): Path to save the inference results (CSV file).\n",
    "        processor: Pretrained processor for the model.\n",
    "        model: Loaded model for inference.\n",
    "        num_frames (int): Number of frames to extract from each video.\n",
    "        device: cpu or gpu.\n",
    "    \"\"\"\n",
    "    # Load video IDs for the first epoch from generated_texts.csv\n",
    "    generated_texts = pd.read_csv(generated_texts_csv)\n",
    "    first_epoch_video_ids = generated_texts[generated_texts['epoch'] == 1]['video_id'].unique()\n",
    "\n",
    "    # Load the annotations and filter for video IDs in the first epoch\n",
    "    annotations = pd.read_csv(csv_file, sep=',')\n",
    "    annotations = annotations[annotations['SENTENCE_NAME'].isin(first_epoch_video_ids)].reset_index(drop=True)\n",
    "\n",
    "    # Create the inference dataset\n",
    "    infer_dataset = VideoDataset(video_dir, annotations, processor, num_frames, \"infer\")\n",
    "\n",
    "    # Check if the output CSV file already exists\n",
    "    file_exists = os.path.exists(output_file)\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(output_file, 'a', newline=\"\") as csvfile:\n",
    "        fieldnames = [\"epoch\", \"id\", \"video_id\", \"generated\", \"true\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header only if the file doesn't already exist\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Inference loop\n",
    "        print(\"Starting inference...\")\n",
    "        for idx in range(len(infer_dataset)):\n",
    "            infer_data = infer_dataset[idx]\n",
    "            # Move inputs to the appropriate device\n",
    "            input_ids = infer_data[\"input_ids\"].to(device).unsqueeze(0)\n",
    "            attention_mask = infer_data[\"attention_mask\"].to(device).unsqueeze(0)\n",
    "            pixel_values_videos = infer_data[\"pixel_values_videos\"].to(device).unsqueeze(0)\n",
    "\n",
    "            # Generate predictions\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values_videos=pixel_values_videos,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "            generated_texts = processor.tokenizer.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            # Clean generated texts and write to CSV\n",
    "            for _, text in enumerate(generated_texts):\n",
    "                keyword = \"ASSISTANT:\"\n",
    "                if keyword in text:\n",
    "                    text = text.split(keyword, 1)[1].strip()\n",
    "\n",
    "                writer.writerow({\n",
    "                    \"epoch\": 1.0,\n",
    "                    \"id\": idx,\n",
    "                    \"video_id\": infer_data['video_id'],\n",
    "                    \"generated\": text,\n",
    "                    \"true\": infer_data['true_sentence']\n",
    "                })\n",
    "\n",
    "    print(f\"Inference complete. Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59630a63-aa07-40e8-83fd-5f1e986be7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor loaded successfully.\n",
      "Created dataset split with 300 entries\n",
      "Starting inference...\n",
      "Inference complete. Results saved to ./output/inference_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "processor.image_processor.do_rescale = False\n",
    "processor.video_processor.do_rescale = False\n",
    "\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model and processor loaded successfully.\")\n",
    "\n",
    "run_inference_with_dataset(\n",
    "    video_dir=VIDEO_DIR,\n",
    "    csv_file=CSV_FILE,\n",
    "    generated_texts_csv=GENERATED_TEXTS,\n",
    "    output_file=OUTPUT_FILE,\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    num_frames=NUM_FRAMES,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055e40a-9aab-464c-a71b-44f9a2045bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
