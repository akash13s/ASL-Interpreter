{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "690d3482-7528-4e5e-a729-5dcaa94f5867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import bisect\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, VideoLlavaForConditionalGeneration\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "import json\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Tuple, Any\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f14bc2-ff30-47ba-b037-d4fbd7e4bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573245db-e723-425a-8a0f-86669408ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd2479c-4267-4bf1-b381-eed847635031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_ID = \"LanguageBind/Video-LLaVA-7B-hf\"\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[-1]\n",
    "\n",
    "# File/directory\n",
    "VIDEO_DIR = \"/scratch/as18464/raw_videos\"\n",
    "CSV_FILE = \"valid_clips.csv\"\n",
    "CACHE_DIR = \"cache/\"\n",
    "DATASET_SIZE = 100\n",
    "\n",
    "# LoRA hyperparameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "    \"k_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "]\n",
    "\n",
    "# model constants\n",
    "BATCH_SIZE = 4\n",
    "MAX_LENGTH = 3500\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e38aabea-0308-46ac-ad51-a946ab26a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "\n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            # Convert to numpy array in RGB format\n",
    "            frame_array = frame.to_ndarray(format=\"rgb24\")\n",
    "            # Apply resize transform and convert back to numpy\n",
    "            resized_frame = resize_transform(frame_array).numpy()\n",
    "            # Convert from CxHxW to HxWxC format and scale back to 0-255 range\n",
    "            resized_frame = (resized_frame.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "            frames.append(resized_frame)\n",
    "    \n",
    "    return np.stack(frames)\n",
    "\n",
    "def get_frames(video_path: str, num_frames: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract frames from video with consistent sampling\n",
    "    Args:\n",
    "        video_path (str): Path to video file\n",
    "        num_frames (int): Number of frames to extract\n",
    "    Returns:\n",
    "        np.ndarray: Array of frames with shape (num_frames, height, width, 3)\n",
    "    \"\"\"\n",
    "    container = av.open(video_path)\n",
    "    \n",
    "    # Get video stream\n",
    "    stream = container.streams.video[0]\n",
    "    total_frames = stream.frames\n",
    "    fps = stream.average_rate\n",
    "    \n",
    "    # Calculate indices to sample\n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    # Read frames at calculated indices\n",
    "    frames = read_video_pyav(container, indices)\n",
    "    \n",
    "    # Ensure we got exactly num_frames\n",
    "    if len(frames) < num_frames:\n",
    "        # If we got fewer frames, duplicate the last frame\n",
    "        last_frame = frames[-1]\n",
    "        while len(frames) < num_frames:\n",
    "            frames = np.concatenate([frames, last_frame[np.newaxis, ...]], axis=0)\n",
    "    elif len(frames) > num_frames:\n",
    "        # If we got more frames, take the first num_frames\n",
    "        frames = frames[:num_frames]\n",
    "    \n",
    "    container.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c13b63-06cb-4714-9971-28db875632b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_dir: str, csv_file: str, num_frames: int = 8):\n",
    "        self.video_dir = video_dir\n",
    "        self.annotations = pd.read_csv(csv_file, sep=',').head(DATASET_SIZE).reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        print(f\"Loaded dataset with {len(self.annotations)} entries\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[str, np.ndarray]:\n",
    "        row = self.annotations.iloc[idx]\n",
    "        video_id = str(row['SENTENCE_NAME']).strip()\n",
    "        sentence = str(row['SENTENCE']).strip()\n",
    "        \n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "        if not os.path.isfile(video_path):\n",
    "            raise FileNotFoundError(f\"Video file '{video_path}' not found.\")\n",
    "        \n",
    "        frames = get_frames(video_path, self.num_frames)\n",
    "\n",
    "        tmp_prompt = \"Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\"\n",
    "        \n",
    "        prompt = f\"USER: <video> {tmp_prompt}\\nASSISTANT: Answer: {sentence}\"\n",
    "\n",
    "        frames_list = [frame for frame in frames]\n",
    "        frames_list = [transforms.ToTensor()(frame) for frame in frames_list]\n",
    "        frame_tensor = torch.stack(frames_list)\n",
    "        \n",
    "        return prompt, frame_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54cb71d-f347-43ca-adfa-295145e376e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(video_dir, csv_file, batch_size, num_frames=8):\n",
    "    dataset = VideoDataset(\n",
    "        video_dir=video_dir,\n",
    "        csv_file=csv_file,\n",
    "        num_frames=num_frames\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for debugging\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8791e6e6-3ec8-4bb7-b81b-2b48e89bcca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10f1129c-20c9-4806-a344-05a2d72efa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b41aeb64-6682-4296-977d-31b069321dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path, accelerator):\n",
    "    # First create base model\n",
    "    model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    # Prepare model with LoRA (same as during training)\n",
    "    p_model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    p_model = get_peft_model(p_model, peft_config)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Load only the LoRA weights\n",
    "    p_model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    \n",
    "    return p_model, checkpoint['epoch'], checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96800503-902c-4eb9-91af-e5b13f36c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2afd8bfd2e45f597b86a68fa1c8fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54449767/ipykernel_2908543/4054281672.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "accelerator = Accelerator()\n",
    "model, epoch, loss = load_trained_model('/scratch/as18464/checkpoint_epoch_18', accelerator)\n",
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "817b7281-3889-48ff-8fed-d2d95c9f0170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2311ada8-4073-4229-8050-f30f898dc7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0310, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "671f9d27-ba8c-438a-a65f-404903c0fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "processor.image_processor.do_rescale = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7352bbf9-1226-482a-a950-bd520e7d775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:  Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\n",
      " ASSISTANT: Answer: Consider. US.Ð‰. US. US. US.............\n"
     ]
    }
   ],
   "source": [
    "video_path = '/scratch/as18464/raw_videos/-08ZGCviCm4_3-5-rgb_front.mp4'\n",
    "container = av.open(video_path)\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, 16).astype(int)\n",
    "clip = read_video_pyav(container, indices)\n",
    "\n",
    "tmp_prompt = \"Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\"        \n",
    "prompt = f\"USER: <video> {tmp_prompt}\\n ASSISTANT: Answer:\"\n",
    "\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_length=100)\n",
    "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b280c-8c1c-4800-a89a-be3e97d595ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
