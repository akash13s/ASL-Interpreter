{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28753c2e-35c5-4fcc-9596-28b3c88bf241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import bisect\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, VideoLlavaForConditionalGeneration\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "import json\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Tuple, Any\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bcd6c8-401a-4cdf-8763-d2f95d18d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61eb68b-02ad-4b46-99a0-7d6ec5d2f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8390602a-89c1-4d31-9e36-9709d04563dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_ID = \"LanguageBind/Video-LLaVA-7B-hf\"\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79eb8919-1f1e-4fa5-9da8-230af5612b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File/directory\n",
    "VIDEO_DIR = \"/scratch/as18464/raw_videos\"\n",
    "CSV_FILE = \"valid_clips.csv\"\n",
    "CACHE_DIR = \"cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7cd0e41-9d4e-4d11-b99a-3da58366da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA hyperparameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "    \"k_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a338078-390b-4aec-b2bc-23555483ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def get_frames(video_path: str, num_frames: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract frames from video with consistent sampling\n",
    "    Args:\n",
    "        video_path (str): Path to video file\n",
    "        num_frames (int): Number of frames to extract\n",
    "    Returns:\n",
    "        np.ndarray: Array of frames with shape (num_frames, height, width, 3)\n",
    "    \"\"\"\n",
    "    container = av.open(video_path)\n",
    "    \n",
    "    # Get video stream\n",
    "    stream = container.streams.video[0]\n",
    "    total_frames = stream.frames\n",
    "    fps = stream.average_rate\n",
    "    \n",
    "    # Calculate indices to sample\n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    # Read frames at calculated indices\n",
    "    frames = read_video_pyav(container, indices)\n",
    "    \n",
    "    # Ensure we got exactly num_frames\n",
    "    if len(frames) < num_frames:\n",
    "        # If we got fewer frames, duplicate the last frame\n",
    "        last_frame = frames[-1]\n",
    "        while len(frames) < num_frames:\n",
    "            frames = np.concatenate([frames, last_frame[np.newaxis, ...]], axis=0)\n",
    "    elif len(frames) > num_frames:\n",
    "        # If we got more frames, take the first num_frames\n",
    "        frames = frames[:num_frames]\n",
    "    \n",
    "    container.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b398c58-95c8-4f62-bf10-b70af2285ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_dir: str, csv_file: str, num_frames: int = 8):\n",
    "        self.video_dir = video_dir\n",
    "        self.annotations = pd.read_csv(csv_file, sep=',').reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        print(f\"Loaded dataset with {len(self.annotations)} entries\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[str, np.ndarray]:\n",
    "        row = self.annotations.iloc[idx]\n",
    "        video_id = str(row['SENTENCE_NAME']).strip()\n",
    "        sentence = str(row['SENTENCE']).strip()\n",
    "        \n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "        if not os.path.isfile(video_path):\n",
    "            raise FileNotFoundError(f\"Video file '{video_path}' not found.\")\n",
    "        \n",
    "        frames = get_frames(video_path, self.num_frames)\n",
    "\n",
    "        tmp_prompt = \"Translate the American Sign Language (ASL) demonstrated in the video to English text, where each frame shows ASL signs used at different time points chronologically.\"\n",
    "        \n",
    "        prompt = f\"USER: <video> {tmp_prompt}\\nASSISTANT: Answer: {sentence}\"\n",
    "\n",
    "        frames_list = [frame for frame in frames]\n",
    "        frames_list = [transforms.ToTensor()(frame) for frame in frames_list]\n",
    "        frame_tensor = torch.stack(frames_list)\n",
    "        \n",
    "        return prompt, frame_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd47b961-241b-421c-8571-cded17646f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model constants\n",
    "BATCH_SIZE = 2\n",
    "MAX_LENGTH = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c1034b8-32c1-450f-9b05-4e41045fe13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, processor, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (texts, videos) in enumerate(progress_bar):\n",
    "        vids = list(torch.unbind(videos, dim=0))\n",
    "        image_lists = []\n",
    "        for batch in vids:\n",
    "            images = [img.permute(1, 2, 0).numpy() for img in batch]\n",
    "            image_lists.append(images)\n",
    "        try:\n",
    "            batch = processor(\n",
    "                text=texts,\n",
    "                videos=image_lists,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            pixel_values_videos = batch[\"pixel_values_videos\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values_videos=pixel_values_videos,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_gradnorm(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
    "            \n",
    "            if epoch%2 == 1:\n",
    "                checkpoint_path = f\"output/checkpoint_epoch_{epoch}\"\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss\n",
    "                }\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e19fde6-78f7-4d17-bf16-7e25eb30db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "def create_data_loader(video_dir, csv_file, batch_size, num_frames=8):\n",
    "    dataset = VideoDataset(\n",
    "        video_dir=video_dir,\n",
    "        csv_file=csv_file,\n",
    "        num_frames=num_frames\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for debugging\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b60140-1f92-4277-9868-da8e31c9f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21374262-b982-4925-8450-f4b0f0c678c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 30760 entries\n"
     ]
    }
   ],
   "source": [
    "train_loader = create_data_loader(\n",
    "    video_dir=VIDEO_DIR,\n",
    "    csv_file=CSV_FILE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_frames=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f551cffb-b995-497a-ba2a-d32eca167fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db1e77ec-c925-466d-bc9e-abed9674cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd7ce1b0ade43689de72a42152d13f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28b249ca-4e60-4478-807a-f41f4c0f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "750f374c-0e9f-4ab8-b6fb-5209787743cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b7195db-8484-47ae-942d-6c0928eb14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22,347,776 || all params: 7,388,626,944 || trainable%: 0.3025\n"
     ]
    }
   ],
   "source": [
    "# Get PEFT model\n",
    "p_model = get_peft_model(p_model, peft_config)\n",
    "p_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9023df98-bf94-4dcf-8d77-485b226158f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(p_model.parameters(), lr=1e-3)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30edc14a-c46f-410f-a125-fcdec46e7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   0%|          | 0/15380 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images/video frames. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Expanding inputs for image tokens in Video-LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.44.\n",
      "/ext3/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Expanding inputs for image tokens in Video-LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Training Epoch 0:   0%|          | 5/15380 [08:17<429:56:41, 100.67s/it, loss=8.89]"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    train_epoch(p_model, train_loader, optimizer, processor, device, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82028a4c-9de9-484c-8d25-0d5b43176f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
