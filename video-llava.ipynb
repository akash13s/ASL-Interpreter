{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28753c2e-35c5-4fcc-9596-28b3c88bf241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import bisect\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, VideoLlavaForConditionalGeneration\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "import json\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Tuple, Any\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast\n",
    "# from torch.nn.parallel import DataParallel\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bcd6c8-401a-4cdf-8763-d2f95d18d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61eb68b-02ad-4b46-99a0-7d6ec5d2f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8390602a-89c1-4d31-9e36-9709d04563dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_ID = \"LanguageBind/Video-LLaVA-7B-hf\"\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79eb8919-1f1e-4fa5-9da8-230af5612b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File/directory\n",
    "VIDEO_DIR = \"/scratch/as18464/raw_videos\"\n",
    "CSV_FILE = \"valid_clips.csv\"\n",
    "CACHE_DIR = \"cache/\"\n",
    "DATASET_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7cd0e41-9d4e-4d11-b99a-3da58366da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA hyperparameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "    \"k_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad4c721-54fa-463b-bfec-44fe6b721927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model constants\n",
    "BATCH_SIZE = 4\n",
    "MAX_LENGTH = 3500\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a338078-390b-4aec-b2bc-23555483ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "\n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            # Convert to numpy array in RGB format\n",
    "            frame_array = frame.to_ndarray(format=\"rgb24\")\n",
    "            # Apply resize transform and convert back to numpy\n",
    "            resized_frame = resize_transform(frame_array).numpy()\n",
    "            # Convert from CxHxW to HxWxC format and scale back to 0-255 range\n",
    "            resized_frame = (resized_frame.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "            frames.append(resized_frame)\n",
    "    \n",
    "    return np.stack(frames)\n",
    "\n",
    "def get_frames(video_path: str, num_frames: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract frames from video with consistent sampling\n",
    "    Args:\n",
    "        video_path (str): Path to video file\n",
    "        num_frames (int): Number of frames to extract\n",
    "    Returns:\n",
    "        np.ndarray: Array of frames with shape (num_frames, height, width, 3)\n",
    "    \"\"\"\n",
    "    container = av.open(video_path)\n",
    "    \n",
    "    # Get video stream\n",
    "    stream = container.streams.video[0]\n",
    "    total_frames = stream.frames\n",
    "    fps = stream.average_rate\n",
    "    \n",
    "    # Calculate indices to sample\n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    # Read frames at calculated indices\n",
    "    frames = read_video_pyav(container, indices)\n",
    "    \n",
    "    # Ensure we got exactly num_frames\n",
    "    if len(frames) < num_frames:\n",
    "        # If we got fewer frames, duplicate the last frame\n",
    "        last_frame = frames[-1]\n",
    "        while len(frames) < num_frames:\n",
    "            frames = np.concatenate([frames, last_frame[np.newaxis, ...]], axis=0)\n",
    "    elif len(frames) > num_frames:\n",
    "        # If we got more frames, take the first num_frames\n",
    "        frames = frames[:num_frames]\n",
    "    \n",
    "    container.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b398c58-95c8-4f62-bf10-b70af2285ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_dir: str, csv_file: str, num_frames: int = 8):\n",
    "        self.video_dir = video_dir\n",
    "        self.annotations = pd.read_csv(csv_file, sep=',').head(DATASET_SIZE).reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        print(f\"Loaded dataset with {len(self.annotations)} entries\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[str, np.ndarray]:\n",
    "        row = self.annotations.iloc[idx]\n",
    "        video_id = str(row['SENTENCE_NAME']).strip()\n",
    "        sentence = str(row['SENTENCE']).strip()\n",
    "        \n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "        if not os.path.isfile(video_path):\n",
    "            raise FileNotFoundError(f\"Video file '{video_path}' not found.\")\n",
    "        \n",
    "        frames = get_frames(video_path, self.num_frames)\n",
    "\n",
    "        tmp_prompt = \"Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\"        \n",
    "        \n",
    "        prompt = f\"USER: <video> {tmp_prompt}\\nASSISTANT: Answer: {sentence}\"\n",
    "\n",
    "        frames_list = [frame for frame in frames]\n",
    "        frames_list = [transforms.ToTensor()(frame) for frame in frames_list]\n",
    "        frame_tensor = torch.stack(frames_list)\n",
    "        \n",
    "        return prompt, frame_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c1034b8-32c1-450f-9b05-4e41045fe13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, processor, accelerator, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (texts, videos) in enumerate(progress_bar):\n",
    "        vids = list(torch.unbind(videos, dim=0))\n",
    "        image_lists = []\n",
    "        for batch in vids:\n",
    "            images = [img.cpu().permute(1, 2, 0).numpy() for img in batch]\n",
    "            image_lists.append(images)\n",
    "        try:\n",
    "            batch = processor(\n",
    "                text=texts,\n",
    "                videos=image_lists,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "            \n",
    "            input_ids = accelerator.prepare(batch[\"input_ids\"])\n",
    "            attention_mask = accelerator.prepare(batch[\"attention_mask\"])\n",
    "            pixel_values_videos = accelerator.prepare(batch[\"pixel_values_videos\"])\n",
    "            labels = accelerator.prepare(batch[\"labels\"])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values_videos=pixel_values_videos,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            # torch.nn.utils.clip_gradnorm(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'batch_loss': f'{current_loss:.4f}',\n",
    "                'avg_loss': f'{avg_loss:.4f}'\n",
    "            })\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                print(f'Epoch {epoch} | Batch {batch_idx}/{len(train_loader)} | '\n",
    "                      f'Loss: {current_loss:.4f} | Avg Loss: {avg_loss:.4f}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        checkpoint_path = f\"output/checkpoint_epoch_{epoch}\"\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        \n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        \n",
    "        if hasattr(unwrapped_model, 'get_peft_state_dict'):\n",
    "            state_dict = unwrapped_model.get_peft_state_dict()\n",
    "        else:\n",
    "            state_dict = unwrapped_model.state_dict()\n",
    "    \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': state_dict,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "        }\n",
    "        \n",
    "        print(f\"Saving checkpoint for epoch {epoch} with average loss: {avg_loss:.4f}\")\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e19fde6-78f7-4d17-bf16-7e25eb30db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "def create_data_loader(video_dir, csv_file, batch_size, num_frames=8):\n",
    "    dataset = VideoDataset(\n",
    "        video_dir=video_dir,\n",
    "        csv_file=csv_file,\n",
    "        num_frames=num_frames\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for debugging\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b60140-1f92-4277-9868-da8e31c9f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21374262-b982-4925-8450-f4b0f0c678c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 4 entries\n"
     ]
    }
   ],
   "source": [
    "train_loader = create_data_loader(\n",
    "    video_dir=VIDEO_DIR,\n",
    "    csv_file=CSV_FILE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_frames=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f551cffb-b995-497a-ba2a-d32eca167fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db1e77ec-c925-466d-bc9e-abed9674cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55372b5d85b43378e4e53c0c495fe05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28b249ca-4e60-4478-807a-f41f4c0f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "750f374c-0e9f-4ab8-b6fb-5209787743cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b7195db-8484-47ae-942d-6c0928eb14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22,347,776 || all params: 7,388,626,944 || trainable%: 0.3025\n"
     ]
    }
   ],
   "source": [
    "# Get PEFT model\n",
    "p_model = get_peft_model(p_model, peft_config)\n",
    "p_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1229a10-51fa-46ef-8958-1cb944983f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainable_params(model):\n",
    "    # First make sure all parameters are not trainable\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Then enable training only for the LoRA parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name:  # This targets only the LoRA layers\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db3e31c0-6458-472f-b977-1d70f4d0ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable_params(p_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "553a5481-67aa-476e-9914-a5b6928e2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(p_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fa57743-d0c3-4984-9300-a5c63a02c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the accelerator with the right kwargs\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08290481-dd8c-41fa-9841-b25970ff49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your model, optimizer, and dataloader with accelerator\n",
    "p_model, optimizer, train_loader = accelerator.prepare(\n",
    "    p_model, optimizer, train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9023df98-bf94-4dcf-8d77-485b226158f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "processor.image_processor.do_rescale = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30edc14a-c46f-410f-a125-fcdec46e7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]Expanding inputs for image tokens in Video-LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.44.\n",
      "/ext3/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Expanding inputs for image tokens in Video-LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Training Epoch 0: 100%|██████████| 1/1 [01:46<00:00, 106.07s/it, batch_loss=3.5275, avg_loss=3.5275]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Batch 0/1 | Loss: 3.5275 | Avg Loss: 3.5275\n",
      "Saving checkpoint for epoch 0 with average loss: 3.5275\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    train_epoch(p_model, train_loader, optimizer, processor, accelerator, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb7316-bb56-433b-b1f9-cc5c1bc18d8c",
   "metadata": {},
   "source": [
    "### Inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8381f5aa-0a68-4982-8f31-18123ea7a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path, accelerator):\n",
    "    # First create base model\n",
    "    model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    # Prepare model with LoRA (same as during training)\n",
    "    p_model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    p_model = get_peft_model(p_model, peft_config)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Load only the LoRA weights\n",
    "    p_model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    \n",
    "    return p_model, checkpoint['epoch'], checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aeadc5a6-0ff3-4317-ae26-a08e92663c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c642248b5da447b8013b2baaf8923ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54356199/ipykernel_1145290/4054281672.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "accelerator = Accelerator()\n",
    "model, epoch, loss = load_trained_model('output/checkpoint_epoch_0', accelerator)\n",
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e74e8d06-c534-4d9d-8bea-828c4fd5dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e7a1b2e-d58e-4dda-8a4b-9872653e7ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5275, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "651e53a7-a7be-43b6-8ebf-cc5f2f7d1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_for_single_video(model, processor, video_path, accelerator):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get frames from the video\n",
    "    frames = get_frames(video_path, num_frames=16)  # Using 16 frames as in training\n",
    "    \n",
    "    # Convert frames to tensor\n",
    "    frames_list = [transforms.ToTensor()(frame) for frame in frames]\n",
    "    frame_tensor = torch.stack(frames_list)\n",
    "    \n",
    "    # Convert to format expected by processor\n",
    "    images = [img.permute(1, 2, 0).cpu().numpy() for img in frame_tensor]\n",
    "    \n",
    "    # Create prompt\n",
    "    tmp_prompt = \"Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\"        \n",
    "    prompt = f\"USER: <video> {tmp_prompt}\\nASSISTANT: Answer:\"\n",
    "    \n",
    "    # Process inputs\n",
    "    batch = processor(\n",
    "        text=prompt,\n",
    "        videos=[images],  # Wrap in list as processor expects batch\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Prepare inputs\n",
    "    input_ids = accelerator.prepare(batch[\"input_ids\"])\n",
    "    attention_mask = accelerator.prepare(batch[\"attention_mask\"])\n",
    "    pixel_values_videos = accelerator.prepare(batch[\"pixel_values_videos\"])\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values_videos=pixel_values_videos,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8a5e76b-8fac-4165-a1cd-4887d0199f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "processor.image_processor.do_rescale = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b03577b8-08a5-4f99-9349-e7cea6bf54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'test/--7E2sU6zP4_11-5-rgb_front.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "754bc879-aa05-4b94-a78d-a8af9af1173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_for_single_video(model, processor, video_path, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f540939c-b1bc-45b6-8796-94bd2f455ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER:  Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\\nASSISTANT: Answer: ThekЪ ,in[ЉЉ.ЉЊЬ (ЉЁ14 in\\nЩ also FFFF\\nЪ and\\nЁc\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c016c-f5c1-4e47-9efa-f6fded6e22b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
