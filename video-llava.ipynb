{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28753c2e-35c5-4fcc-9596-28b3c88bf241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import bisect\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, VideoLlavaForConditionalGeneration\n",
    "# from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "import json\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Tuple, Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e61eb68b-02ad-4b46-99a0-7d6ec5d2f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8390602a-89c1-4d31-9e36-9709d04563dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_ID = \"LanguageBind/Video-LLaVA-7B-hf\"\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79eb8919-1f1e-4fa5-9da8-230af5612b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File/directory\n",
    "VIDEO_DIR = \"./test\"\n",
    "CSV_FILE = \"./test/dummy.csv\"\n",
    "CACHE_DIR = \"./cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a338078-390b-4aec-b2bc-23555483ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def get_frames(video_path: str, num_frames: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract frames from video with consistent sampling\n",
    "    Args:\n",
    "        video_path (str): Path to video file\n",
    "        num_frames (int): Number of frames to extract\n",
    "    Returns:\n",
    "        np.ndarray: Array of frames with shape (num_frames, height, width, 3)\n",
    "    \"\"\"\n",
    "    container = av.open(video_path)\n",
    "    \n",
    "    # Get video stream\n",
    "    stream = container.streams.video[0]\n",
    "    total_frames = stream.frames\n",
    "    fps = stream.average_rate\n",
    "    \n",
    "    # Calculate indices to sample\n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    # Read frames at calculated indices\n",
    "    frames = read_video_pyav(container, indices)\n",
    "    \n",
    "    # Ensure we got exactly num_frames\n",
    "    if len(frames) < num_frames:\n",
    "        # If we got fewer frames, duplicate the last frame\n",
    "        last_frame = frames[-1]\n",
    "        while len(frames) < num_frames:\n",
    "            frames = np.concatenate([frames, last_frame[np.newaxis, ...]], axis=0)\n",
    "    elif len(frames) > num_frames:\n",
    "        # If we got more frames, take the first num_frames\n",
    "        frames = frames[:num_frames]\n",
    "    \n",
    "    container.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b398c58-95c8-4f62-bf10-b70af2285ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_dir: str, csv_file: str, num_frames: int = 8):\n",
    "        self.video_dir = video_dir\n",
    "        self.annotations = pd.read_csv(csv_file, sep=',').reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        print(f\"Loaded dataset with {len(self.annotations)} entries\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[str, np.ndarray]:\n",
    "        row = self.annotations.iloc[idx]\n",
    "        video_id = str(row['SENTENCE_NAME']).strip()\n",
    "        sentence = str(row['SENTENCE']).strip()\n",
    "        \n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "        if not os.path.isfile(video_path):\n",
    "            raise FileNotFoundError(f\"Video file '{video_path}' not found.\")\n",
    "        \n",
    "        frames = get_frames(video_path, self.num_frames)\n",
    "\n",
    "        tmp_prompt = \"Translate the sign language in the video to English text.\"\n",
    "        \n",
    "        prompt = f\"USER: <video> {tmp_prompt}\\nASSISTANT: Answer: {sentence}\"\n",
    "\n",
    "        frames_list = [frame for frame in frames]\n",
    "        frames_list = [transforms.ToTensor()(frame) for frame in frames_list]\n",
    "        frame_tensor = torch.stack(frames_list)\n",
    "        \n",
    "        return prompt, frame_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd47b961-241b-421c-8571-cded17646f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model constants\n",
    "BATCH_SIZE = 2\n",
    "MAX_LENGTH = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c1034b8-32c1-450f-9b05-4e41045fe13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, processor, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (texts, videos) in enumerate(progress_bar):\n",
    "        vids = list(torch.unbind(videos, dim=0))\n",
    "        image_lists = []\n",
    "        for batch in vids:\n",
    "            images = [img.permute(1, 2, 0).numpy() for img in batch]  # Permute to [720, 1280, 3]\n",
    "            image_lists.append(images)\n",
    "\n",
    "            print(len(image_lists))\n",
    "            print(len(image_lists[0]))\n",
    "\n",
    "        try:\n",
    "            # Process the batch\n",
    "            batch = processor(\n",
    "                text=texts,\n",
    "                videos=image_lists,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "            \n",
    "            # Move everything to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            pixel_values_videos = batch[\"pixel_values_videos\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values_videos=pixel_values_videos,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e19fde6-78f7-4d17-bf16-7e25eb30db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "def create_data_loader(video_dir, csv_file, batch_size, num_frames=8):\n",
    "    dataset = VideoDataset(\n",
    "        video_dir=video_dir,\n",
    "        csv_file=csv_file,\n",
    "        num_frames=num_frames\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for debugging\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b60140-1f92-4277-9868-da8e31c9f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21374262-b982-4925-8450-f4b0f0c678c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 4 entries\n"
     ]
    }
   ],
   "source": [
    "train_loader = create_data_loader(\n",
    "    video_dir=VIDEO_DIR,\n",
    "    csv_file=CSV_FILE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_frames=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f8c8d34-4feb-4264-942c-af6c43a678f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1b9fe89-dbed-4be0-a6e7-ecbeb908e5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 3, 720, 1280])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f551cffb-b995-497a-ba2a-d32eca167fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db1e77ec-c925-466d-bc9e-abed9674cb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b96e65de6e54a36969a96b6ae2d20b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dbf69d0d6b4898a131317b25530d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/112k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa2e3abbde04390b8bc585c731753b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ac511d811040689c8c66bad46579d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fe4f193b294416a5dc87daa3df27f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89fc3db216b4443a7409658d86ee25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVideoLlavaForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCACHE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/transformers/modeling_utils.py:4174\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4171\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[1;32m   4173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4174\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4176\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4177\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:102\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 102\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         )\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023df98-bf94-4dcf-8d77-485b226158f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30edc14a-c46f-410f-a125-fcdec46e7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    train_epoch(model, train_loader, optimizer, processor, device, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b90b2-de9e-42f6-b901-95f8e914154b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
