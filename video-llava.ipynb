{
 "cells": [
  {
   "cell_type": "code",
   "id": "28753c2e-35c5-4fcc-9596-28b3c88bf241",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import av\n",
    "import bisect\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, VideoLlavaForConditionalGeneration\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "import json\n",
    "import torch.nn.utils.prune as prune\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from typing import Tuple, Any\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast\n",
    "# from torch.nn.parallel import DataParallel\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bcd6c8-401a-4cdf-8763-d2f95d18d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61eb68b-02ad-4b46-99a0-7d6ec5d2f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8390602a-89c1-4d31-9e36-9709d04563dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_ID = \"LanguageBind/Video-LLaVA-7B-hf\"\n",
    "MODEL_NAME = MODEL_ID.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79eb8919-1f1e-4fa5-9da8-230af5612b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File/directory\n",
    "VIDEO_DIR = \"/scratch/as18464/raw_videos\"\n",
    "CSV_FILE = \"valid_clips.csv\"\n",
    "CACHE_DIR = \"cache/\"\n",
    "DATASET_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7cd0e41-9d4e-4d11-b99a-3da58366da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA hyperparameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "    \"k_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad4c721-54fa-463b-bfec-44fe6b721927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model constants\n",
    "BATCH_SIZE = 4\n",
    "MAX_LENGTH = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a338078-390b-4aec-b2bc-23555483ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "\n",
    "    resize_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            # Convert to numpy array in RGB format\n",
    "            frame_array = frame.to_ndarray(format=\"rgb24\")\n",
    "            # Apply resize transform and convert back to numpy\n",
    "            resized_frame = resize_transform(frame_array).numpy()\n",
    "            # Convert from CxHxW to HxWxC format and scale back to 0-255 range\n",
    "            resized_frame = (resized_frame.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
    "            frames.append(resized_frame)\n",
    "    \n",
    "    return np.stack(frames)\n",
    "\n",
    "def get_frames(video_path: str, num_frames: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract frames from video with consistent sampling\n",
    "    Args:\n",
    "        video_path (str): Path to video file\n",
    "        num_frames (int): Number of frames to extract\n",
    "    Returns:\n",
    "        np.ndarray: Array of frames with shape (num_frames, height, width, 3)\n",
    "    \"\"\"\n",
    "    container = av.open(video_path)\n",
    "    \n",
    "    # Get video stream\n",
    "    stream = container.streams.video[0]\n",
    "    total_frames = stream.frames\n",
    "    fps = stream.average_rate\n",
    "    \n",
    "    # Calculate indices to sample\n",
    "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    # Read frames at calculated indices\n",
    "    frames = read_video_pyav(container, indices)\n",
    "    \n",
    "    # Ensure we got exactly num_frames\n",
    "    if len(frames) < num_frames:\n",
    "        # If we got fewer frames, duplicate the last frame\n",
    "        last_frame = frames[-1]\n",
    "        while len(frames) < num_frames:\n",
    "            frames = np.concatenate([frames, last_frame[np.newaxis, ...]], axis=0)\n",
    "    elif len(frames) > num_frames:\n",
    "        # If we got more frames, take the first num_frames\n",
    "        frames = frames[:num_frames]\n",
    "    \n",
    "    container.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b398c58-95c8-4f62-bf10-b70af2285ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_dir: str, csv_file: str, num_frames: int = 8):\n",
    "        self.video_dir = video_dir\n",
    "        self.annotations = pd.read_csv(csv_file, sep=',').head(DATASET_SIZE).reset_index(drop=True)\n",
    "        self.num_frames = num_frames\n",
    "        self.system_prompt = (\"Analyze the American Sign Language (ASL) signs in this video and \"\n",
    "                            \"translate them into clear, natural English. Consider the sequence of \"\n",
    "                            \"signs as a complete message, and provide an accurate translation that \"\n",
    "                            \"captures the full meaning. Respond with only the English translation, \"\n",
    "                            \"without descriptions of the signs themselves.\")\n",
    "        \n",
    "        print(f\"Loaded dataset with {len(self.annotations)} entries\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[str, np.ndarray]:\n",
    "        row = self.annotations.iloc[idx]\n",
    "        video_id = str(row['SENTENCE_NAME']).strip()\n",
    "        sentence = str(row['SENTENCE']).strip()\n",
    "        \n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "        if not os.path.isfile(video_path):\n",
    "            raise FileNotFoundError(f\"Video file '{video_path}' not found.\")\n",
    "        \n",
    "        frames = get_frames(video_path, self.num_frames)\n",
    "        \n",
    "        prompt = f\"USER: {self.system_prompt}\\n<video>\\nASSISTANT: {sentence}\"\n",
    "\n",
    "        frames_list = [frame for frame in frames]\n",
    "        frames_list = [transforms.ToTensor()(frame) for frame in frames_list]\n",
    "        frame_tensor = torch.stack(frames_list)\n",
    "        \n",
    "        return prompt, frame_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c1034b8-32c1-450f-9b05-4e41045fe13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, processor, accelerator, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (texts, videos) in enumerate(progress_bar):\n",
    "        vids = list(torch.unbind(videos, dim=0))\n",
    "        image_lists = []\n",
    "        for batch in vids:\n",
    "            images = [img.cpu().permute(1, 2, 0).numpy() for img in batch]\n",
    "            image_lists.append(images)\n",
    "        try:\n",
    "            batch = processor(\n",
    "                text=texts,\n",
    "                videos=image_lists,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "            for i, text in enumerate(texts):\n",
    "                assistant_start = None\n",
    "                # Look for sequence: \"ASSISTANT:\"\n",
    "                for j in range(len(batch[\"input_ids\"][i])):\n",
    "                    if processor.tokenizer.decode(batch[\"input_ids\"][i][j:j+4]) == \"ASSISTANT:\":\n",
    "                        assistant_start = j\n",
    "                        break\n",
    "                \n",
    "                if assistant_start is not None:\n",
    "                    # Mask everything before and including \"ASSISTANT:\"\n",
    "                    labels[i, :assistant_start+4] = -100\n",
    "\n",
    "            # To remove later - for debugging\n",
    "            # print(\"\\n====== Tokens and Labels for Batch\", batch_idx, \"======\")\n",
    "            for i, text in enumerate(texts):\n",
    "                print(f\"\\nOriginal text {i}: {text}\")\n",
    "                print(\"\\nTokens and their labels:\")\n",
    "                tokens = processor.tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][i])\n",
    "                for j, (token, label) in enumerate(zip(tokens, labels[i])):\n",
    "                    print(f\"Position {j:3d} | Token: {token:15} | Label: {label.item():5}\")\n",
    "                print(\"-\" * 50)\n",
    "            \n",
    "            batch[\"labels\"] = labels\n",
    "            \n",
    "            input_ids = accelerator.prepare(batch[\"input_ids\"])\n",
    "            attention_mask = accelerator.prepare(batch[\"attention_mask\"])\n",
    "            pixel_values_videos = accelerator.prepare(batch[\"pixel_values_videos\"])\n",
    "            labels = accelerator.prepare(batch[\"labels\"])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values_videos=pixel_values_videos,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            # torch.nn.utils.clip_gradnorm(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'batch_loss': f'{current_loss:.4f}',\n",
    "                'avg_loss': f'{avg_loss:.4f}'\n",
    "            })\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                print(f'Epoch {epoch} | Batch {batch_idx}/{len(train_loader)} | '\n",
    "                      f'Loss: {current_loss:.4f} | Avg Loss: {avg_loss:.4f}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    if accelerator.is_main_process and epoch%5 == 0:\n",
    "        checkpoint_path = f\"output/checkpoint_epoch_{epoch}\"\n",
    "        os.makedirs(\"output\", exist_ok=True)\n",
    "        \n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        \n",
    "        if hasattr(unwrapped_model, 'get_peft_state_dict'):\n",
    "            state_dict = unwrapped_model.get_peft_state_dict()\n",
    "        else:\n",
    "            state_dict = unwrapped_model.state_dict()\n",
    "    \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': state_dict,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "        }\n",
    "        \n",
    "        print(f\"Saving checkpoint for epoch {epoch} with average loss: {avg_loss:.4f}\")\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e19fde6-78f7-4d17-bf16-7e25eb30db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "def create_data_loader(video_dir, csv_file, batch_size, num_frames=8):\n",
    "    dataset = VideoDataset(\n",
    "        video_dir=video_dir,\n",
    "        csv_file=csv_file,\n",
    "        num_frames=num_frames\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for debugging\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1b60140-1f92-4277-9868-da8e31c9f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21374262-b982-4925-8450-f4b0f0c678c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 4 entries\n"
     ]
    }
   ],
   "source": [
    "train_loader = create_data_loader(\n",
    "    video_dir=VIDEO_DIR,\n",
    "    csv_file=CSV_FILE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_frames=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f551cffb-b995-497a-ba2a-d32eca167fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db1e77ec-c925-466d-bc9e-abed9674cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d4c30483384bdea68cd3a14eab5523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28b249ca-4e60-4478-807a-f41f4c0f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "750f374c-0e9f-4ab8-b6fb-5209787743cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b7195db-8484-47ae-942d-6c0928eb14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 22,347,776 || all params: 7,388,626,944 || trainable%: 0.3025\n"
     ]
    }
   ],
   "source": [
    "# Get PEFT model\n",
    "p_model = get_peft_model(p_model, peft_config)\n",
    "p_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1229a10-51fa-46ef-8958-1cb944983f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainable_params(model):\n",
    "    # First make sure all parameters are not trainable\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Then enable training only for the LoRA parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name:  # This targets only the LoRA layers\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db3e31c0-6458-472f-b977-1d70f4d0ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_trainable_params(p_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "553a5481-67aa-476e-9914-a5b6928e2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(p_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fa57743-d0c3-4984-9300-a5c63a02c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the accelerator with the right kwargs\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08290481-dd8c-41fa-9841-b25970ff49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your model, optimizer, and dataloader with accelerator\n",
    "p_model, optimizer, train_loader = accelerator.prepare(\n",
    "    p_model, optimizer, train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9023df98-bf94-4dcf-8d77-485b226158f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "processor.image_processor.do_rescale = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30edc14a-c46f-410f-a125-fcdec46e7005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text 0: USER: Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\n",
      "<video>\n",
      "ASSISTANT: This is all the you know, take off on the idea of the acanthus leaf.\n",
      "\n",
      "Tokens and their labels:\n",
      "Position   0 | Token: <s>             | Label:  -100\n",
      "Position   1 | Token: ▁US             | Label:  -100\n",
      "Position   2 | Token: ER              | Label:  -100\n",
      "Position   3 | Token: :               | Label:  -100\n",
      "Position   4 | Token: ▁Anal           | Label:  -100\n",
      "Position   5 | Token: y               | Label:  -100\n",
      "Position   6 | Token: ze              | Label:  -100\n",
      "Position   7 | Token: ▁the            | Label:  -100\n",
      "Position   8 | Token: ▁American       | Label:  -100\n",
      "Position   9 | Token: ▁Sign           | Label:  -100\n",
      "Position  10 | Token: ▁Language       | Label:  -100\n",
      "Position  11 | Token: ▁(              | Label:  -100\n",
      "Position  12 | Token: AS              | Label:  -100\n",
      "Position  13 | Token: L               | Label:  -100\n",
      "Position  14 | Token: )               | Label:  -100\n",
      "Position  15 | Token: ▁signs          | Label:  -100\n",
      "Position  16 | Token: ▁in             | Label:  -100\n",
      "Position  17 | Token: ▁this           | Label:  -100\n",
      "Position  18 | Token: ▁video          | Label:  -100\n",
      "Position  19 | Token: ▁and            | Label:  -100\n",
      "Position  20 | Token: ▁translate      | Label:  -100\n",
      "Position  21 | Token: ▁them           | Label:  -100\n",
      "Position  22 | Token: ▁into           | Label:  -100\n",
      "Position  23 | Token: ▁clear          | Label:  -100\n",
      "Position  24 | Token: ,               | Label:  -100\n",
      "Position  25 | Token: ▁natural        | Label:  -100\n",
      "Position  26 | Token: ▁English        | Label:  -100\n",
      "Position  27 | Token: .               | Label:  -100\n",
      "Position  28 | Token: ▁Consider       | Label:  -100\n",
      "Position  29 | Token: ▁the            | Label:  -100\n",
      "Position  30 | Token: ▁sequence       | Label:  -100\n",
      "Position  31 | Token: ▁of             | Label:  -100\n",
      "Position  32 | Token: ▁signs          | Label:  -100\n",
      "Position  33 | Token: ▁as             | Label:  -100\n",
      "Position  34 | Token: ▁a              | Label:  -100\n",
      "Position  35 | Token: ▁complete       | Label:  -100\n",
      "Position  36 | Token: ▁message        | Label:  -100\n",
      "Position  37 | Token: ,               | Label:  -100\n",
      "Position  38 | Token: ▁and            | Label:  -100\n",
      "Position  39 | Token: ▁provide        | Label:  -100\n",
      "Position  40 | Token: ▁an             | Label:  -100\n",
      "Position  41 | Token: ▁accurate       | Label:  -100\n",
      "Position  42 | Token: ▁translation    | Label:  -100\n",
      "Position  43 | Token: ▁that           | Label:  -100\n",
      "Position  44 | Token: ▁capt           | Label:  -100\n",
      "Position  45 | Token: ures            | Label:  -100\n",
      "Position  46 | Token: ▁the            | Label:  -100\n",
      "Position  47 | Token: ▁full           | Label:  -100\n",
      "Position  48 | Token: ▁meaning        | Label:  -100\n",
      "Position  49 | Token: .               | Label:  -100\n",
      "Position  50 | Token: ▁Res            | Label:  -100\n",
      "Position  51 | Token: pond            | Label:  -100\n",
      "Position  52 | Token: ▁with           | Label:  -100\n",
      "Position  53 | Token: ▁only           | Label:  -100\n",
      "Position  54 | Token: ▁the            | Label:  -100\n",
      "Position  55 | Token: ▁English        | Label:  -100\n",
      "Position  56 | Token: ▁translation    | Label:  -100\n",
      "Position  57 | Token: ,               | Label:  -100\n",
      "Position  58 | Token: ▁without        | Label:  -100\n",
      "Position  59 | Token: ▁descri         | Label:  -100\n",
      "Position  60 | Token: ptions          | Label:  -100\n",
      "Position  61 | Token: ▁of             | Label:  -100\n",
      "Position  62 | Token: ▁the            | Label:  -100\n",
      "Position  63 | Token: ▁signs          | Label:  -100\n",
      "Position  64 | Token: ▁themselves     | Label:  -100\n",
      "Position  65 | Token: .               | Label:  -100\n",
      "Position  66 | Token: <0x0A>          | Label:  -100\n",
      "Position  67 | Token: <video>         | Label:  -100\n",
      "Position  68 | Token: <0x0A>          | Label:  -100\n",
      "Position  69 | Token: ASS             | Label:  -100\n",
      "Position  70 | Token: IST             | Label:  -100\n",
      "Position  71 | Token: ANT             | Label:  -100\n",
      "Position  72 | Token: :               | Label:  -100\n",
      "Position  73 | Token: ▁This           | Label:   910\n",
      "Position  74 | Token: ▁is             | Label:   338\n",
      "Position  75 | Token: ▁all            | Label:   599\n",
      "Position  76 | Token: ▁the            | Label:   278\n",
      "Position  77 | Token: ▁you            | Label:   366\n",
      "Position  78 | Token: ▁know           | Label:  1073\n",
      "Position  79 | Token: ,               | Label: 29892\n",
      "Position  80 | Token: ▁take           | Label:  2125\n",
      "Position  81 | Token: ▁off            | Label:  1283\n",
      "Position  82 | Token: ▁on             | Label:   373\n",
      "Position  83 | Token: ▁the            | Label:   278\n",
      "Position  84 | Token: ▁idea           | Label:  2969\n",
      "Position  85 | Token: ▁of             | Label:   310\n",
      "Position  86 | Token: ▁the            | Label:   278\n",
      "Position  87 | Token: ▁ac             | Label:  1274\n",
      "Position  88 | Token: anth            | Label:  9716\n",
      "Position  89 | Token: us              | Label:   375\n",
      "Position  90 | Token: ▁leaf           | Label: 20447\n",
      "Position  91 | Token: .               | Label: 29889\n",
      "Position  92 | Token: <pad>           | Label:  -100\n",
      "Position  93 | Token: <pad>           | Label:  -100\n",
      "Position  94 | Token: <pad>           | Label:  -100\n",
      "Position  95 | Token: <pad>           | Label:  -100\n",
      "Position  96 | Token: <pad>           | Label:  -100\n",
      "Position  97 | Token: <pad>           | Label:  -100\n",
      "Position  98 | Token: <pad>           | Label:  -100\n",
      "Position  99 | Token: <pad>           | Label:  -100\n",
      "Position 100 | Token: <pad>           | Label:  -100\n",
      "Position 101 | Token: <pad>           | Label:  -100\n",
      "Position 102 | Token: <pad>           | Label:  -100\n",
      "Position 103 | Token: <pad>           | Label:  -100\n",
      "Position 104 | Token: <pad>           | Label:  -100\n",
      "Position 105 | Token: <pad>           | Label:  -100\n",
      "Position 106 | Token: <pad>           | Label:  -100\n",
      "Position 107 | Token: <pad>           | Label:  -100\n",
      "Position 108 | Token: <pad>           | Label:  -100\n",
      "Position 109 | Token: <pad>           | Label:  -100\n",
      "Position 110 | Token: <pad>           | Label:  -100\n",
      "Position 111 | Token: <pad>           | Label:  -100\n",
      "Position 112 | Token: <pad>           | Label:  -100\n",
      "Position 113 | Token: <pad>           | Label:  -100\n",
      "Position 114 | Token: <pad>           | Label:  -100\n",
      "Position 115 | Token: <pad>           | Label:  -100\n",
      "Position 116 | Token: <pad>           | Label:  -100\n",
      "Position 117 | Token: <pad>           | Label:  -100\n",
      "Position 118 | Token: <pad>           | Label:  -100\n",
      "Position 119 | Token: <pad>           | Label:  -100\n",
      "Position 120 | Token: <pad>           | Label:  -100\n",
      "Position 121 | Token: <pad>           | Label:  -100\n",
      "Position 122 | Token: <pad>           | Label:  -100\n",
      "Position 123 | Token: <pad>           | Label:  -100\n",
      "Position 124 | Token: <pad>           | Label:  -100\n",
      "Position 125 | Token: <pad>           | Label:  -100\n",
      "--------------------------------------------------\n",
      "\n",
      "Original text 1: USER: Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\n",
      "<video>\n",
      "ASSISTANT: So they don't really have much of a symbolic meaning other than maybe life is richer, life is beautiful, but they've become so beautifully stylized and so you find them in different illuminative being rendered in very different ways.\n",
      "\n",
      "Tokens and their labels:\n",
      "Position   0 | Token: <s>             | Label:  -100\n",
      "Position   1 | Token: ▁US             | Label:  -100\n",
      "Position   2 | Token: ER              | Label:  -100\n",
      "Position   3 | Token: :               | Label:  -100\n",
      "Position   4 | Token: ▁Anal           | Label:  -100\n",
      "Position   5 | Token: y               | Label:  -100\n",
      "Position   6 | Token: ze              | Label:  -100\n",
      "Position   7 | Token: ▁the            | Label:  -100\n",
      "Position   8 | Token: ▁American       | Label:  -100\n",
      "Position   9 | Token: ▁Sign           | Label:  -100\n",
      "Position  10 | Token: ▁Language       | Label:  -100\n",
      "Position  11 | Token: ▁(              | Label:  -100\n",
      "Position  12 | Token: AS              | Label:  -100\n",
      "Position  13 | Token: L               | Label:  -100\n",
      "Position  14 | Token: )               | Label:  -100\n",
      "Position  15 | Token: ▁signs          | Label:  -100\n",
      "Position  16 | Token: ▁in             | Label:  -100\n",
      "Position  17 | Token: ▁this           | Label:  -100\n",
      "Position  18 | Token: ▁video          | Label:  -100\n",
      "Position  19 | Token: ▁and            | Label:  -100\n",
      "Position  20 | Token: ▁translate      | Label:  -100\n",
      "Position  21 | Token: ▁them           | Label:  -100\n",
      "Position  22 | Token: ▁into           | Label:  -100\n",
      "Position  23 | Token: ▁clear          | Label:  -100\n",
      "Position  24 | Token: ,               | Label:  -100\n",
      "Position  25 | Token: ▁natural        | Label:  -100\n",
      "Position  26 | Token: ▁English        | Label:  -100\n",
      "Position  27 | Token: .               | Label:  -100\n",
      "Position  28 | Token: ▁Consider       | Label:  -100\n",
      "Position  29 | Token: ▁the            | Label:  -100\n",
      "Position  30 | Token: ▁sequence       | Label:  -100\n",
      "Position  31 | Token: ▁of             | Label:  -100\n",
      "Position  32 | Token: ▁signs          | Label:  -100\n",
      "Position  33 | Token: ▁as             | Label:  -100\n",
      "Position  34 | Token: ▁a              | Label:  -100\n",
      "Position  35 | Token: ▁complete       | Label:  -100\n",
      "Position  36 | Token: ▁message        | Label:  -100\n",
      "Position  37 | Token: ,               | Label:  -100\n",
      "Position  38 | Token: ▁and            | Label:  -100\n",
      "Position  39 | Token: ▁provide        | Label:  -100\n",
      "Position  40 | Token: ▁an             | Label:  -100\n",
      "Position  41 | Token: ▁accurate       | Label:  -100\n",
      "Position  42 | Token: ▁translation    | Label:  -100\n",
      "Position  43 | Token: ▁that           | Label:  -100\n",
      "Position  44 | Token: ▁capt           | Label:  -100\n",
      "Position  45 | Token: ures            | Label:  -100\n",
      "Position  46 | Token: ▁the            | Label:  -100\n",
      "Position  47 | Token: ▁full           | Label:  -100\n",
      "Position  48 | Token: ▁meaning        | Label:  -100\n",
      "Position  49 | Token: .               | Label:  -100\n",
      "Position  50 | Token: ▁Res            | Label:  -100\n",
      "Position  51 | Token: pond            | Label:  -100\n",
      "Position  52 | Token: ▁with           | Label:  -100\n",
      "Position  53 | Token: ▁only           | Label:  -100\n",
      "Position  54 | Token: ▁the            | Label:  -100\n",
      "Position  55 | Token: ▁English        | Label:  -100\n",
      "Position  56 | Token: ▁translation    | Label:  -100\n",
      "Position  57 | Token: ,               | Label:  -100\n",
      "Position  58 | Token: ▁without        | Label:  -100\n",
      "Position  59 | Token: ▁descri         | Label:  -100\n",
      "Position  60 | Token: ptions          | Label:  -100\n",
      "Position  61 | Token: ▁of             | Label:  -100\n",
      "Position  62 | Token: ▁the            | Label:  -100\n",
      "Position  63 | Token: ▁signs          | Label:  -100\n",
      "Position  64 | Token: ▁themselves     | Label:  -100\n",
      "Position  65 | Token: .               | Label:  -100\n",
      "Position  66 | Token: <0x0A>          | Label:  -100\n",
      "Position  67 | Token: <video>         | Label:  -100\n",
      "Position  68 | Token: <0x0A>          | Label:  -100\n",
      "Position  69 | Token: ASS             | Label:  -100\n",
      "Position  70 | Token: IST             | Label:  -100\n",
      "Position  71 | Token: ANT             | Label:  -100\n",
      "Position  72 | Token: :               | Label:  -100\n",
      "Position  73 | Token: ▁So             | Label:  1105\n",
      "Position  74 | Token: ▁they           | Label:   896\n",
      "Position  75 | Token: ▁don            | Label:  1016\n",
      "Position  76 | Token: '               | Label: 29915\n",
      "Position  77 | Token: t               | Label: 29873\n",
      "Position  78 | Token: ▁really         | Label:  2289\n",
      "Position  79 | Token: ▁have           | Label:   505\n",
      "Position  80 | Token: ▁much           | Label:  1568\n",
      "Position  81 | Token: ▁of             | Label:   310\n",
      "Position  82 | Token: ▁a              | Label:   263\n",
      "Position  83 | Token: ▁symbol         | Label:  5829\n",
      "Position  84 | Token: ic              | Label:   293\n",
      "Position  85 | Token: ▁meaning        | Label:  6593\n",
      "Position  86 | Token: ▁other          | Label:   916\n",
      "Position  87 | Token: ▁than           | Label:  1135\n",
      "Position  88 | Token: ▁maybe          | Label:  5505\n",
      "Position  89 | Token: ▁life           | Label:  2834\n",
      "Position  90 | Token: ▁is             | Label:   338\n",
      "Position  91 | Token: ▁rich           | Label:  8261\n",
      "Position  92 | Token: er              | Label:   261\n",
      "Position  93 | Token: ,               | Label: 29892\n",
      "Position  94 | Token: ▁life           | Label:  2834\n",
      "Position  95 | Token: ▁is             | Label:   338\n",
      "Position  96 | Token: ▁beautiful      | Label:  9560\n",
      "Position  97 | Token: ,               | Label: 29892\n",
      "Position  98 | Token: ▁but            | Label:   541\n",
      "Position  99 | Token: ▁they           | Label:   896\n",
      "Position 100 | Token: '               | Label: 29915\n",
      "Position 101 | Token: ve              | Label:   345\n",
      "Position 102 | Token: ▁become         | Label:  4953\n",
      "Position 103 | Token: ▁so             | Label:   577\n",
      "Position 104 | Token: ▁beautiful      | Label:  9560\n",
      "Position 105 | Token: ly              | Label:   368\n",
      "Position 106 | Token: ▁st             | Label:   380\n",
      "Position 107 | Token: yl              | Label:  2904\n",
      "Position 108 | Token: ized            | Label:  1891\n",
      "Position 109 | Token: ▁and            | Label:   322\n",
      "Position 110 | Token: ▁so             | Label:   577\n",
      "Position 111 | Token: ▁you            | Label:   366\n",
      "Position 112 | Token: ▁find           | Label:  1284\n",
      "Position 113 | Token: ▁them           | Label:   963\n",
      "Position 114 | Token: ▁in             | Label:   297\n",
      "Position 115 | Token: ▁different      | Label:  1422\n",
      "Position 116 | Token: ▁ill            | Label:  4486\n",
      "Position 117 | Token: umin            | Label:  9735\n",
      "Position 118 | Token: ative           | Label:  1230\n",
      "Position 119 | Token: ▁being          | Label:  1641\n",
      "Position 120 | Token: ▁rendered       | Label: 13751\n",
      "Position 121 | Token: ▁in             | Label:   297\n",
      "Position 122 | Token: ▁very           | Label:  1407\n",
      "Position 123 | Token: ▁different      | Label:  1422\n",
      "Position 124 | Token: ▁ways           | Label:  5837\n",
      "Position 125 | Token: .               | Label: 29889\n",
      "--------------------------------------------------\n",
      "\n",
      "Original text 2: USER: Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\n",
      "<video>\n",
      "ASSISTANT: And I call them decorative elements because basically all they're meant to do is to enrich and color the page.\n",
      "\n",
      "Tokens and their labels:\n",
      "Position   0 | Token: <s>             | Label:  -100\n",
      "Position   1 | Token: ▁US             | Label:  -100\n",
      "Position   2 | Token: ER              | Label:  -100\n",
      "Position   3 | Token: :               | Label:  -100\n",
      "Position   4 | Token: ▁Anal           | Label:  -100\n",
      "Position   5 | Token: y               | Label:  -100\n",
      "Position   6 | Token: ze              | Label:  -100\n",
      "Position   7 | Token: ▁the            | Label:  -100\n",
      "Position   8 | Token: ▁American       | Label:  -100\n",
      "Position   9 | Token: ▁Sign           | Label:  -100\n",
      "Position  10 | Token: ▁Language       | Label:  -100\n",
      "Position  11 | Token: ▁(              | Label:  -100\n",
      "Position  12 | Token: AS              | Label:  -100\n",
      "Position  13 | Token: L               | Label:  -100\n",
      "Position  14 | Token: )               | Label:  -100\n",
      "Position  15 | Token: ▁signs          | Label:  -100\n",
      "Position  16 | Token: ▁in             | Label:  -100\n",
      "Position  17 | Token: ▁this           | Label:  -100\n",
      "Position  18 | Token: ▁video          | Label:  -100\n",
      "Position  19 | Token: ▁and            | Label:  -100\n",
      "Position  20 | Token: ▁translate      | Label:  -100\n",
      "Position  21 | Token: ▁them           | Label:  -100\n",
      "Position  22 | Token: ▁into           | Label:  -100\n",
      "Position  23 | Token: ▁clear          | Label:  -100\n",
      "Position  24 | Token: ,               | Label:  -100\n",
      "Position  25 | Token: ▁natural        | Label:  -100\n",
      "Position  26 | Token: ▁English        | Label:  -100\n",
      "Position  27 | Token: .               | Label:  -100\n",
      "Position  28 | Token: ▁Consider       | Label:  -100\n",
      "Position  29 | Token: ▁the            | Label:  -100\n",
      "Position  30 | Token: ▁sequence       | Label:  -100\n",
      "Position  31 | Token: ▁of             | Label:  -100\n",
      "Position  32 | Token: ▁signs          | Label:  -100\n",
      "Position  33 | Token: ▁as             | Label:  -100\n",
      "Position  34 | Token: ▁a              | Label:  -100\n",
      "Position  35 | Token: ▁complete       | Label:  -100\n",
      "Position  36 | Token: ▁message        | Label:  -100\n",
      "Position  37 | Token: ,               | Label:  -100\n",
      "Position  38 | Token: ▁and            | Label:  -100\n",
      "Position  39 | Token: ▁provide        | Label:  -100\n",
      "Position  40 | Token: ▁an             | Label:  -100\n",
      "Position  41 | Token: ▁accurate       | Label:  -100\n",
      "Position  42 | Token: ▁translation    | Label:  -100\n",
      "Position  43 | Token: ▁that           | Label:  -100\n",
      "Position  44 | Token: ▁capt           | Label:  -100\n",
      "Position  45 | Token: ures            | Label:  -100\n",
      "Position  46 | Token: ▁the            | Label:  -100\n",
      "Position  47 | Token: ▁full           | Label:  -100\n",
      "Position  48 | Token: ▁meaning        | Label:  -100\n",
      "Position  49 | Token: .               | Label:  -100\n",
      "Position  50 | Token: ▁Res            | Label:  -100\n",
      "Position  51 | Token: pond            | Label:  -100\n",
      "Position  52 | Token: ▁with           | Label:  -100\n",
      "Position  53 | Token: ▁only           | Label:  -100\n",
      "Position  54 | Token: ▁the            | Label:  -100\n",
      "Position  55 | Token: ▁English        | Label:  -100\n",
      "Position  56 | Token: ▁translation    | Label:  -100\n",
      "Position  57 | Token: ,               | Label:  -100\n",
      "Position  58 | Token: ▁without        | Label:  -100\n",
      "Position  59 | Token: ▁descri         | Label:  -100\n",
      "Position  60 | Token: ptions          | Label:  -100\n",
      "Position  61 | Token: ▁of             | Label:  -100\n",
      "Position  62 | Token: ▁the            | Label:  -100\n",
      "Position  63 | Token: ▁signs          | Label:  -100\n",
      "Position  64 | Token: ▁themselves     | Label:  -100\n",
      "Position  65 | Token: .               | Label:  -100\n",
      "Position  66 | Token: <0x0A>          | Label:  -100\n",
      "Position  67 | Token: <video>         | Label:  -100\n",
      "Position  68 | Token: <0x0A>          | Label:  -100\n",
      "Position  69 | Token: ASS             | Label:  -100\n",
      "Position  70 | Token: IST             | Label:  -100\n",
      "Position  71 | Token: ANT             | Label:  -100\n",
      "Position  72 | Token: :               | Label:  -100\n",
      "Position  73 | Token: ▁And            | Label:  1126\n",
      "Position  74 | Token: ▁I              | Label:   306\n",
      "Position  75 | Token: ▁call           | Label:  1246\n",
      "Position  76 | Token: ▁them           | Label:   963\n",
      "Position  77 | Token: ▁decor          | Label: 10200\n",
      "Position  78 | Token: ative           | Label:  1230\n",
      "Position  79 | Token: ▁elements       | Label:  3161\n",
      "Position  80 | Token: ▁because        | Label:  1363\n",
      "Position  81 | Token: ▁basically      | Label:  8830\n",
      "Position  82 | Token: ▁all            | Label:   599\n",
      "Position  83 | Token: ▁they           | Label:   896\n",
      "Position  84 | Token: '               | Label: 29915\n",
      "Position  85 | Token: re              | Label:   276\n",
      "Position  86 | Token: ▁meant          | Label:  6839\n",
      "Position  87 | Token: ▁to             | Label:   304\n",
      "Position  88 | Token: ▁do             | Label:   437\n",
      "Position  89 | Token: ▁is             | Label:   338\n",
      "Position  90 | Token: ▁to             | Label:   304\n",
      "Position  91 | Token: ▁en             | Label:   427\n",
      "Position  92 | Token: rich            | Label:  4018\n",
      "Position  93 | Token: ▁and            | Label:   322\n",
      "Position  94 | Token: ▁color          | Label:  2927\n",
      "Position  95 | Token: ▁the            | Label:   278\n",
      "Position  96 | Token: ▁page           | Label:  1813\n",
      "Position  97 | Token: .               | Label: 29889\n",
      "Position  98 | Token: <pad>           | Label:  -100\n",
      "Position  99 | Token: <pad>           | Label:  -100\n",
      "Position 100 | Token: <pad>           | Label:  -100\n",
      "Position 101 | Token: <pad>           | Label:  -100\n",
      "Position 102 | Token: <pad>           | Label:  -100\n",
      "Position 103 | Token: <pad>           | Label:  -100\n",
      "Position 104 | Token: <pad>           | Label:  -100\n",
      "Position 105 | Token: <pad>           | Label:  -100\n",
      "Position 106 | Token: <pad>           | Label:  -100\n",
      "Position 107 | Token: <pad>           | Label:  -100\n",
      "Position 108 | Token: <pad>           | Label:  -100\n",
      "Position 109 | Token: <pad>           | Label:  -100\n",
      "Position 110 | Token: <pad>           | Label:  -100\n",
      "Position 111 | Token: <pad>           | Label:  -100\n",
      "Position 112 | Token: <pad>           | Label:  -100\n",
      "Position 113 | Token: <pad>           | Label:  -100\n",
      "Position 114 | Token: <pad>           | Label:  -100\n",
      "Position 115 | Token: <pad>           | Label:  -100\n",
      "Position 116 | Token: <pad>           | Label:  -100\n",
      "Position 117 | Token: <pad>           | Label:  -100\n",
      "Position 118 | Token: <pad>           | Label:  -100\n",
      "Position 119 | Token: <pad>           | Label:  -100\n",
      "Position 120 | Token: <pad>           | Label:  -100\n",
      "Position 121 | Token: <pad>           | Label:  -100\n",
      "Position 122 | Token: <pad>           | Label:  -100\n",
      "Position 123 | Token: <pad>           | Label:  -100\n",
      "Position 124 | Token: <pad>           | Label:  -100\n",
      "Position 125 | Token: <pad>           | Label:  -100\n",
      "--------------------------------------------------\n",
      "\n",
      "Original text 3: USER: Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\n",
      "<video>\n",
      "ASSISTANT: Now this is very, this is actually an insert of a kind of an envelope for stationary, and this is a very Italian design.\n",
      "\n",
      "Tokens and their labels:\n",
      "Position   0 | Token: <s>             | Label:  -100\n",
      "Position   1 | Token: ▁US             | Label:  -100\n",
      "Position   2 | Token: ER              | Label:  -100\n",
      "Position   3 | Token: :               | Label:  -100\n",
      "Position   4 | Token: ▁Anal           | Label:  -100\n",
      "Position   5 | Token: y               | Label:  -100\n",
      "Position   6 | Token: ze              | Label:  -100\n",
      "Position   7 | Token: ▁the            | Label:  -100\n",
      "Position   8 | Token: ▁American       | Label:  -100\n",
      "Position   9 | Token: ▁Sign           | Label:  -100\n",
      "Position  10 | Token: ▁Language       | Label:  -100\n",
      "Position  11 | Token: ▁(              | Label:  -100\n",
      "Position  12 | Token: AS              | Label:  -100\n",
      "Position  13 | Token: L               | Label:  -100\n",
      "Position  14 | Token: )               | Label:  -100\n",
      "Position  15 | Token: ▁signs          | Label:  -100\n",
      "Position  16 | Token: ▁in             | Label:  -100\n",
      "Position  17 | Token: ▁this           | Label:  -100\n",
      "Position  18 | Token: ▁video          | Label:  -100\n",
      "Position  19 | Token: ▁and            | Label:  -100\n",
      "Position  20 | Token: ▁translate      | Label:  -100\n",
      "Position  21 | Token: ▁them           | Label:  -100\n",
      "Position  22 | Token: ▁into           | Label:  -100\n",
      "Position  23 | Token: ▁clear          | Label:  -100\n",
      "Position  24 | Token: ,               | Label:  -100\n",
      "Position  25 | Token: ▁natural        | Label:  -100\n",
      "Position  26 | Token: ▁English        | Label:  -100\n",
      "Position  27 | Token: .               | Label:  -100\n",
      "Position  28 | Token: ▁Consider       | Label:  -100\n",
      "Position  29 | Token: ▁the            | Label:  -100\n",
      "Position  30 | Token: ▁sequence       | Label:  -100\n",
      "Position  31 | Token: ▁of             | Label:  -100\n",
      "Position  32 | Token: ▁signs          | Label:  -100\n",
      "Position  33 | Token: ▁as             | Label:  -100\n",
      "Position  34 | Token: ▁a              | Label:  -100\n",
      "Position  35 | Token: ▁complete       | Label:  -100\n",
      "Position  36 | Token: ▁message        | Label:  -100\n",
      "Position  37 | Token: ,               | Label:  -100\n",
      "Position  38 | Token: ▁and            | Label:  -100\n",
      "Position  39 | Token: ▁provide        | Label:  -100\n",
      "Position  40 | Token: ▁an             | Label:  -100\n",
      "Position  41 | Token: ▁accurate       | Label:  -100\n",
      "Position  42 | Token: ▁translation    | Label:  -100\n",
      "Position  43 | Token: ▁that           | Label:  -100\n",
      "Position  44 | Token: ▁capt           | Label:  -100\n",
      "Position  45 | Token: ures            | Label:  -100\n",
      "Position  46 | Token: ▁the            | Label:  -100\n",
      "Position  47 | Token: ▁full           | Label:  -100\n",
      "Position  48 | Token: ▁meaning        | Label:  -100\n",
      "Position  49 | Token: .               | Label:  -100\n",
      "Position  50 | Token: ▁Res            | Label:  -100\n",
      "Position  51 | Token: pond            | Label:  -100\n",
      "Position  52 | Token: ▁with           | Label:  -100\n",
      "Position  53 | Token: ▁only           | Label:  -100\n",
      "Position  54 | Token: ▁the            | Label:  -100\n",
      "Position  55 | Token: ▁English        | Label:  -100\n",
      "Position  56 | Token: ▁translation    | Label:  -100\n",
      "Position  57 | Token: ,               | Label:  -100\n",
      "Position  58 | Token: ▁without        | Label:  -100\n",
      "Position  59 | Token: ▁descri         | Label:  -100\n",
      "Position  60 | Token: ptions          | Label:  -100\n",
      "Position  61 | Token: ▁of             | Label:  -100\n",
      "Position  62 | Token: ▁the            | Label:  -100\n",
      "Position  63 | Token: ▁signs          | Label:  -100\n",
      "Position  64 | Token: ▁themselves     | Label:  -100\n",
      "Position  65 | Token: .               | Label:  -100\n",
      "Position  66 | Token: <0x0A>          | Label:  -100\n",
      "Position  67 | Token: <video>         | Label:  -100\n",
      "Position  68 | Token: <0x0A>          | Label:  -100\n",
      "Position  69 | Token: ASS             | Label:  -100\n",
      "Position  70 | Token: IST             | Label:  -100\n",
      "Position  71 | Token: ANT             | Label:  -100\n",
      "Position  72 | Token: :               | Label:  -100\n",
      "Position  73 | Token: ▁Now            | Label:  2567\n",
      "Position  74 | Token: ▁this           | Label:   445\n",
      "Position  75 | Token: ▁is             | Label:   338\n",
      "Position  76 | Token: ▁very           | Label:  1407\n",
      "Position  77 | Token: ,               | Label: 29892\n",
      "Position  78 | Token: ▁this           | Label:   445\n",
      "Position  79 | Token: ▁is             | Label:   338\n",
      "Position  80 | Token: ▁actually       | Label:  2869\n",
      "Position  81 | Token: ▁an             | Label:   385\n",
      "Position  82 | Token: ▁insert         | Label:  4635\n",
      "Position  83 | Token: ▁of             | Label:   310\n",
      "Position  84 | Token: ▁a              | Label:   263\n",
      "Position  85 | Token: ▁kind           | Label:  2924\n",
      "Position  86 | Token: ▁of             | Label:   310\n",
      "Position  87 | Token: ▁an             | Label:   385\n",
      "Position  88 | Token: ▁en             | Label:   427\n",
      "Position  89 | Token: velope          | Label: 21367\n",
      "Position  90 | Token: ▁for            | Label:   363\n",
      "Position  91 | Token: ▁station        | Label:  5073\n",
      "Position  92 | Token: ary             | Label:   653\n",
      "Position  93 | Token: ,               | Label: 29892\n",
      "Position  94 | Token: ▁and            | Label:   322\n",
      "Position  95 | Token: ▁this           | Label:   445\n",
      "Position  96 | Token: ▁is             | Label:   338\n",
      "Position  97 | Token: ▁a              | Label:   263\n",
      "Position  98 | Token: ▁very           | Label:  1407\n",
      "Position  99 | Token: ▁Italian        | Label: 10545\n",
      "Position 100 | Token: ▁design         | Label:  2874\n",
      "Position 101 | Token: .               | Label: 29889\n",
      "Position 102 | Token: <pad>           | Label:  -100\n",
      "Position 103 | Token: <pad>           | Label:  -100\n",
      "Position 104 | Token: <pad>           | Label:  -100\n",
      "Position 105 | Token: <pad>           | Label:  -100\n",
      "Position 106 | Token: <pad>           | Label:  -100\n",
      "Position 107 | Token: <pad>           | Label:  -100\n",
      "Position 108 | Token: <pad>           | Label:  -100\n",
      "Position 109 | Token: <pad>           | Label:  -100\n",
      "Position 110 | Token: <pad>           | Label:  -100\n",
      "Position 111 | Token: <pad>           | Label:  -100\n",
      "Position 112 | Token: <pad>           | Label:  -100\n",
      "Position 113 | Token: <pad>           | Label:  -100\n",
      "Position 114 | Token: <pad>           | Label:  -100\n",
      "Position 115 | Token: <pad>           | Label:  -100\n",
      "Position 116 | Token: <pad>           | Label:  -100\n",
      "Position 117 | Token: <pad>           | Label:  -100\n",
      "Position 118 | Token: <pad>           | Label:  -100\n",
      "Position 119 | Token: <pad>           | Label:  -100\n",
      "Position 120 | Token: <pad>           | Label:  -100\n",
      "Position 121 | Token: <pad>           | Label:  -100\n",
      "Position 122 | Token: <pad>           | Label:  -100\n",
      "Position 123 | Token: <pad>           | Label:  -100\n",
      "Position 124 | Token: <pad>           | Label:  -100\n",
      "Position 125 | Token: <pad>           | Label:  -100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/1 [00:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccelerator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[42], line 64\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_loader, optimizer, processor, accelerator, epoch)\u001B[0m\n\u001B[1;32m     56\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\n\u001B[1;32m     57\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m     58\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m     59\u001B[0m     pixel_values_videos\u001B[38;5;241m=\u001B[39mpixel_values_videos,\n\u001B[1;32m     60\u001B[0m     labels\u001B[38;5;241m=\u001B[39mlabels\n\u001B[1;32m     61\u001B[0m )\n\u001B[1;32m     62\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[0;32m---> 64\u001B[0m \u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# torch.nn.utils.clip_gradnorm(model.parameters(), 1.0)\u001B[39;00m\n\u001B[1;32m     67\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m/ext3/miniforge3/lib/python3.12/site-packages/accelerate/accelerator.py:2241\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   2239\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[1;32m   2240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2241\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    520\u001B[0m     )\n\u001B[0;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_epoch(p_model, train_loader, optimizer, processor, accelerator, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb7316-bb56-433b-b1f9-cc5c1bc18d8c",
   "metadata": {},
   "source": [
    "### Inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8381f5aa-0a68-4982-8f31-18123ea7a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(checkpoint_path, accelerator):\n",
    "    # First create base model\n",
    "    model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "    \n",
    "    # Prepare model with LoRA (same as during training)\n",
    "    p_model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    p_model = get_peft_model(p_model, peft_config)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Load only the LoRA weights\n",
    "    p_model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    \n",
    "    return p_model, checkpoint['epoch'], checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeadc5a6-0ff3-4317-ae26-a08e92663c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea4bd1bc50e4dbc9d0865125452b4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54535504/ipykernel_4172088/4054281672.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "accelerator = Accelerator()\n",
    "model, epoch, loss = load_trained_model('output/checkpoint_epoch_20', accelerator)\n",
    "model = accelerator.prepare(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e74e8d06-c534-4d9d-8bea-828c4fd5dbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e7a1b2e-d58e-4dda-8a4b-9872653e7ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0271, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "651e53a7-a7be-43b6-8ebf-cc5f2f7d1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_for_single_video(model, processor, video_path, accelerator):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get frames from the video\n",
    "    frames = get_frames(video_path, num_frames=16)  # Using 16 frames as in training\n",
    "    \n",
    "    # Convert frames to tensor\n",
    "    frames_list = [transforms.ToTensor()(frame) for frame in frames]\n",
    "    frame_tensor = torch.stack(frames_list)\n",
    "    \n",
    "    # Convert to format expected by processor\n",
    "    images = [img.permute(1, 2, 0).cpu().numpy() for img in frame_tensor]\n",
    "    \n",
    "    # Create prompt\n",
    "    tmp_prompt = \"Translate the sign language to english text.\"        \n",
    "    prompt = f\"USER: <video> {tmp_prompt}\\n ASSISTANT: Answer:\"\n",
    "    \n",
    "    # Process inputs\n",
    "    batch = processor(\n",
    "        text=prompt,\n",
    "        videos=[images],  # Wrap in list as processor expects batch\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Prepare inputs\n",
    "    input_ids = accelerator.prepare(batch[\"input_ids\"])\n",
    "    attention_mask = accelerator.prepare(batch[\"attention_mask\"])\n",
    "    pixel_values_videos = accelerator.prepare(batch[\"pixel_values_videos\"])\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values_videos=pixel_values_videos,\n",
    "            max_length=200\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = processor.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8a5e76b-8fac-4165-a1cd-4887d0199f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "processor.image_processor.do_rescale = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "754bc879-aa05-4b94-a78d-a8af9af1173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/scratch/as18464/raw_videos/-06_nJnhORg_3-5-rgb_front.mp4'\n",
    "generated_text = generate_for_single_video(model, processor, video_path, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08307b35-d274-4a09-a3bb-967397f4dbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USER:  Translate the sign language to english text.\\n ASSISTANT: Answer: Gener']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f0c016c-f5c1-4e47-9efa-f6fded6e22b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:  Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\n",
      " ASSISTANT: Answer: andЪЪЪЪЪЪЪЪЪЪЪЪЪЪЪЪЪЪЪЪЪЪЪ\n"
     ]
    }
   ],
   "source": [
    "# video_path = 'test/--7E2sU6zP4_12-5-rgb_front.mp4'\n",
    "# container = av.open(video_path)\n",
    "# total_frames = container.streams.video[0].frames\n",
    "# indices = np.arange(0, total_frames, 16).astype(int)\n",
    "# clip = read_video_pyav(container, indices)\n",
    "\n",
    "# tmp_prompt = \"Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\"        \n",
    "# prompt = f\"USER: <video> {tmp_prompt}\\n ASSISTANT: Answer:\"\n",
    "\n",
    "# inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\")\n",
    "\n",
    "# # Generate\n",
    "# generate_ids = model.generate(**inputs, max_length=100)\n",
    "# print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59983b-3d98-4ddb-a0c7-8113e610e057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
