{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash13s/ASL-Interpreter/blob/llava-next/llava_next.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "LxtTk6knbkGh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxtTk6knbkGh",
        "outputId": "a5113c6a-a56d-4be0-cc61-6272fe32a403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (14.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install av numpy torch torchvision peft pandas tqdm\n",
        "!pip install --upgrade bitsandbytes transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "28753c2e-35c5-4fcc-9596-28b3c88bf241",
      "metadata": {
        "id": "28753c2e-35c5-4fcc-9596-28b3c88bf241"
      },
      "outputs": [],
      "source": [
        "import av\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BitsAndBytesConfig, LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from typing import Tuple\n",
        "import pandas as pd\n",
        "from torch.nn.parallel import DataParallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e61eb68b-02ad-4b46-99a0-7d6ec5d2f55a",
      "metadata": {
        "id": "e61eb68b-02ad-4b46-99a0-7d6ec5d2f55a"
      },
      "outputs": [],
      "source": [
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8390602a-89c1-4d31-9e36-9709d04563dd",
      "metadata": {
        "id": "8390602a-89c1-4d31-9e36-9709d04563dd"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "MODEL_ID = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n",
        "MODEL_NAME = MODEL_ID.split(\"/\")[-1]\n",
        "\n",
        "# File/directory\n",
        "VIDEO_DIR = \"/scratch/mg7609/ASL-Interpreter/data/raw_videos\"\n",
        "CSV_FILE = \"/scratch/mg7609/ASL-Interpreter/data/valid_clips.csv\"\n",
        "CACHE_DIR = \"cache/\"\n",
        "DATASET_SIZE = 4\n",
        "\n",
        "# model constants\n",
        "BATCH_SIZE = 1\n",
        "MAX_LENGTH = 3500\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "\n",
        "# LoRA hyperparameters\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.1\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q_proj\",\n",
        "    \"v_proj\",\n",
        "    \"k_proj\",\n",
        "    \"o_proj\",\n",
        "    \"gate_proj\",\n",
        "    \"up_proj\",\n",
        "    \"down_proj\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "LhitpWC147OP",
      "metadata": {
        "id": "LhitpWC147OP"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "kwaa8Erw47oN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "802190fcc1634bc6aee865517ab08ca9",
            "43616b5c31a344658d3bf841b3a3a7fb",
            "9013fd49a4d242e0a61930e12389ee6d",
            "62c7cc659b59467a914b93a9c1f0e6fe",
            "4a6e1f53a9d74f3d9cb562d4f3bffbde",
            "a07a48e8cd4742bb9fd5df929c2dea25",
            "5856f1f526c04c228b567ad0b9965797",
            "46a5051810984d75819638feb3137075",
            "f80aaa8a251e4629a4013b3cf29f0d82",
            "dc89bb2a0a2e47e8b9d3f14e1b5fd178",
            "b0680c62f3e84e49bbe134bcdf96dfbd"
          ]
        },
        "id": "kwaa8Erw47oN",
        "outputId": "9876311e-aad1-4054-de0f-d47fe332c30e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "802190fcc1634bc6aee865517ab08ca9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    cache_dir=CACHE_DIR,\n",
        "    quantization_config=quantization_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6e8bba0",
      "metadata": {},
      "outputs": [],
      "source": [
        "p_model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee9f0bd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050297af",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get PEFT model\n",
        "p_model = get_peft_model(p_model, peft_config)\n",
        "p_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df7ab74",
      "metadata": {},
      "outputs": [],
      "source": [
        "p_model = p_model.cuda()\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "    p_model = DataParallel(p_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "830b1364",
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(p_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "processor = LlavaNextVideoProcessor.from_pretrained(MODEL_ID)\n",
        "processor.tokenizer.padding_side = \"right\"\n",
        "processor.image_processor.do_rescale = False\n",
        "processor.video_processor.do_rescale = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "8a338078-390b-4aec-b2bc-23555483ffd7",
      "metadata": {
        "id": "8a338078-390b-4aec-b2bc-23555483ffd7"
      },
      "outputs": [],
      "source": [
        "def read_video_pyav(container, indices):\n",
        "    '''\n",
        "    Decode the video with PyAV decoder.\n",
        "    Args:\n",
        "        container (`av.container.input.InputContainer`): PyAV container.\n",
        "        indices (`List[int]`): List of frame indices to decode.\n",
        "    Returns:\n",
        "        result (np.ndarray): np array of decoded frames of shape (num_frames, 3, height, width).\n",
        "    '''\n",
        "    frames = []\n",
        "    container.seek(0)\n",
        "    start_index = indices[0]\n",
        "    end_index = indices[-1]\n",
        "\n",
        "    resize_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    for i, frame in enumerate(container.decode(video=0)):\n",
        "        if i > end_index:\n",
        "            break\n",
        "        if i >= start_index and i in indices:\n",
        "            # Convert to numpy array in RGB format\n",
        "            frame_array = frame.to_ndarray(format=\"rgb24\")\n",
        "            # Apply resize transform and convert back to numpy\n",
        "            resized_frame = resize_transform(frame_array).numpy()\n",
        "            # Scale back to 0-255 range\n",
        "            resized_frame = (resized_frame.transpose(1, 2, 0) * 255).astype(np.uint8)\n",
        "            frames.append(resized_frame)\n",
        "\n",
        "    return np.stack(frames)\n",
        "\n",
        "def get_frames(video_path: str, num_frames: int = 8) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract frames from video with consistent sampling\n",
        "    Args:\n",
        "        video_path (str): Path to video file\n",
        "        num_frames (int): Number of frames to extract\n",
        "    Returns:\n",
        "        np.ndarray: Array of frames with shape (num_frames, 3, height, width)\n",
        "    \"\"\"\n",
        "    container = av.open(video_path)\n",
        "\n",
        "    # Get video stream\n",
        "    stream = container.streams.video[0]\n",
        "    total_frames = stream.frames\n",
        "    fps = stream.average_rate\n",
        "\n",
        "    # Calculate indices to sample\n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    # Read frames at calculated indices\n",
        "    frames = read_video_pyav(container, indices)\n",
        "\n",
        "    # Ensure we got exactly num_frames\n",
        "    if len(frames) < num_frames:\n",
        "        # If we got fewer frames, duplicate the last frame\n",
        "        last_frame = frames[-1]\n",
        "        while len(frames) < num_frames:\n",
        "            frames = np.concatenate([frames, last_frame[np.newaxis, ...]], axis=0)\n",
        "    elif len(frames) > num_frames:\n",
        "        # If we got more frames, take the first num_frames\n",
        "        frames = frames[:num_frames]\n",
        "\n",
        "    container.close()\n",
        "\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "7b398c58-95c8-4f62-bf10-b70af2285ce0",
      "metadata": {
        "id": "7b398c58-95c8-4f62-bf10-b70af2285ce0"
      },
      "outputs": [],
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_dir: str, csv_file: str, num_frames: int = 8):\n",
        "        self.video_dir = video_dir\n",
        "        self.annotations = pd.read_csv(csv_file, sep=',').head(DATASET_SIZE).reset_index(drop=True)\n",
        "        self.num_frames = num_frames\n",
        "        print(f\"Loaded dataset with {len(self.annotations)} entries\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[str, np.ndarray]:\n",
        "        row = self.annotations.iloc[idx]\n",
        "        video_id = str(row['SENTENCE_NAME']).strip()\n",
        "        sentence = str(row['SENTENCE']).strip()\n",
        "\n",
        "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
        "        if not os.path.isfile(video_path):\n",
        "            raise FileNotFoundError(f\"Video file '{video_path}' not found.\")\n",
        "\n",
        "        frames = get_frames(video_path, self.num_frames)\n",
        "\n",
        "        tmp_prompt = \"Analyze the American Sign Language (ASL) signs in this video and translate them into clear, natural English. Consider the sequence of signs as a complete message, and provide an accurate translation that captures the full meaning. Respond with only the English translation, without descriptions of the signs themselves.\"\n",
        "        prompt = f\"USER: <video> {tmp_prompt}\\nASSISTANT: Answer: {sentence}\"\n",
        "\n",
        "        frames_list = [frame for frame in frames]\n",
        "        frames_list = [transforms.ToTensor()(frame) for frame in frames_list]\n",
        "        frame_tensor = torch.stack(frames_list)\n",
        "\n",
        "        return prompt, frame_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d9510d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset and dataloader\n",
        "def create_data_loader(video_dir, csv_file, batch_size, num_frames=8):\n",
        "    dataset = VideoDataset(\n",
        "        video_dir=video_dir,\n",
        "        csv_file=csv_file,\n",
        "        num_frames=num_frames\n",
        "    )\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Set to 0 for debugging\n",
        "        pin_memory=False\n",
        "    )\n",
        "\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6fb7017",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = create_data_loader(\n",
        "    video_dir=VIDEO_DIR,\n",
        "    csv_file=CSV_FILE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_frames = 16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "6c1034b8-32c1-450f-9b05-4e41045fe13f",
      "metadata": {
        "id": "6c1034b8-32c1-450f-9b05-4e41045fe13f"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, processor, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n",
        "\n",
        "\n",
        "    for batch_idx, (texts, videos) in enumerate(progress_bar):\n",
        "        vids = list(torch.unbind(videos, dim=0))\n",
        "        image_lists = []\n",
        "        for batch in vids:\n",
        "            images = [img.permute(1, 2, 0).numpy() for img in batch]\n",
        "            image_lists.append(images)\n",
        "        try:\n",
        "            batch = processor(\n",
        "                text=texts,\n",
        "                videos=image_lists,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=MAX_LENGTH,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            labels = batch[\"input_ids\"].clone()\n",
        "            labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "            batch[\"labels\"] = labels\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            pixel_values_videos = batch[\"pixel_values_videos\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            n_video_tokens = (input_ids == processor.tokenizer.convert_tokens_to_ids(\"<video>\")).sum().item()\n",
        "            frame_count = pixel_values_videos.shape[1]\n",
        "            height, width = pixel_values_videos.shape[3], pixel_values_videos.shape[4]\n",
        "            expected_tokens = frame_count * (height // processor.patch_size) * (width // processor.patch_size) // 4\n",
        "            \n",
        "            if n_video_tokens != expected_tokens:\n",
        "                # Adjust attention_mask\n",
        "                adjusted_attention_mask = torch.ones((1, input_ids.size(1) + expected_tokens - n_video_tokens), device=device)\n",
        "                adjusted_attention_mask[:, :input_ids.size(1)] = attention_mask\n",
        "                attention_mask = adjusted_attention_mask\n",
        "\n",
        "                # Adjust input_ids\n",
        "                if n_video_tokens < expected_tokens:\n",
        "                    extra_tokens = expected_tokens - n_video_tokens\n",
        "                    new_tokens = torch.full((1, extra_tokens), processor.tokenizer.convert_tokens_to_ids(\"<video>\"), device=device)\n",
        "                    input_ids = torch.cat([input_ids, new_tokens], dim=-1)\n",
        "                elif n_video_tokens > expected_tokens:\n",
        "                    mask = input_ids != processor.tokenizer.convert_tokens_to_ids(\"<video>\")\n",
        "                    input_ids = input_ids[mask]\n",
        "                    input_ids = input_ids[:, :expected_tokens]  # Truncate to expected length\n",
        "\n",
        "                # Adjust labels\n",
        "                adjusted_labels = torch.full_like(input_ids, -100)  # Start with all tokens ignored\n",
        "                adjusted_labels[:, :labels.size(1)] = labels\n",
        "                labels = adjusted_labels\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                pixel_values_videos=pixel_values_videos,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "\n",
        "            if isinstance(model, DataParallel):\n",
        "                loss = loss.mean()\n",
        "\n",
        "            loss.backward()\n",
        "            # torch.nn.utils.clip_gradnorm(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            current_loss = loss.item()\n",
        "            total_loss += current_loss\n",
        "            avg_loss = total_loss / (batch_idx + 1)\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'batch_loss': f'{current_loss:.4f}',\n",
        "                'avg_loss': f'{avg_loss:.4f}'\n",
        "            })\n",
        "\n",
        "            # Print detailed loss information for each iteration\n",
        "            print(f'Epoch {epoch} | Batch {batch_idx}/{len(train_loader)} | '\n",
        "                  f'Loss: {current_loss:.4f} | Avg Loss: {avg_loss:.4f}')\n",
        "\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "\n",
        "    # Save checkpoint after every epoch\n",
        "    checkpoint_path = f\"output/checkpoint_epoch_{epoch}\"\n",
        "    os.makedirs(\"output\", exist_ok=True)  # Ensure output directory exists\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.module.state_dict() if isinstance(model, DataParallel) else model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_loss\n",
        "    }\n",
        "\n",
        "    print(f\"Saving checkpoint for epoch {epoch} with average loss: {avg_loss:.4f}\")\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "    return total_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30edc14a-c46f-410f-a125-fcdec46e7005",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30edc14a-c46f-410f-a125-fcdec46e7005",
        "outputId": "64593f29-bb4a-4481-c0a5-b22dbb5bbadc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting <video> tokens from 345 to 2304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 0:  25%|██▌       | 1/4 [00:37<01:51, 37.07s/it, batch_loss=4.0374, avg_loss=4.0374]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Batch 0/4 | Loss: 4.0374 | Avg Loss: 4.0374\n",
            "Adjusting <video> tokens from 345 to 2304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 0:  50%|█████     | 2/4 [01:12<01:11, 35.83s/it, batch_loss=4.0029, avg_loss=4.0201]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Batch 1/4 | Loss: 4.0029 | Avg Loss: 4.0201\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    train_epoch(p_model, train_loader, optimizer, processor, device, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82028a4c-9de9-484c-8d25-0d5b43176f8e",
      "metadata": {
        "id": "82028a4c-9de9-484c-8d25-0d5b43176f8e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43616b5c31a344658d3bf841b3a3a7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a07a48e8cd4742bb9fd5df929c2dea25",
            "placeholder": "​",
            "style": "IPY_MODEL_5856f1f526c04c228b567ad0b9965797",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "46a5051810984d75819638feb3137075": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a6e1f53a9d74f3d9cb562d4f3bffbde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5856f1f526c04c228b567ad0b9965797": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62c7cc659b59467a914b93a9c1f0e6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc89bb2a0a2e47e8b9d3f14e1b5fd178",
            "placeholder": "​",
            "style": "IPY_MODEL_b0680c62f3e84e49bbe134bcdf96dfbd",
            "value": " 3/3 [01:08&lt;00:00, 21.95s/it]"
          }
        },
        "802190fcc1634bc6aee865517ab08ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43616b5c31a344658d3bf841b3a3a7fb",
              "IPY_MODEL_9013fd49a4d242e0a61930e12389ee6d",
              "IPY_MODEL_62c7cc659b59467a914b93a9c1f0e6fe"
            ],
            "layout": "IPY_MODEL_4a6e1f53a9d74f3d9cb562d4f3bffbde"
          }
        },
        "9013fd49a4d242e0a61930e12389ee6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a5051810984d75819638feb3137075",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f80aaa8a251e4629a4013b3cf29f0d82",
            "value": 3
          }
        },
        "a07a48e8cd4742bb9fd5df929c2dea25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0680c62f3e84e49bbe134bcdf96dfbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc89bb2a0a2e47e8b9d3f14e1b5fd178": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f80aaa8a251e4629a4013b3cf29f0d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
